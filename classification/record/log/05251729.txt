Note: no_Randomresizecrop 
==================== Dataset Info ==================== 
train images: (5994, 3) 
valid images: (5794, 3) 
num classes: 200 
==================== info ==================== 
model: resnet 

*** Pretext imformation *** 
batch size: 32 
epoch: 512 
with LIO: True 
M: True 
mask/coor loss weight: 0.1 0.1 
loss warmup epochs: 100 
LIO mask size: 7 
attention:  
learning rate: 0.01 
scheduler: cos 
input resolution: 224 
simclr out dim: 128 

*** downstream imformation *** 
batch size: 64 
epoch: 128 
learning rate: 0.01 
scheduler: cos 
input resolution: 224 
pretrain with imagenet: False 

*** record *** 
pretext model save at  ./record/weight/05251729/resnet_pretext.pth 
downstream model save at  ./record/weight/05251729/resnet_downstream.pth 
log write at  ./record/log/05251729.txt 
mask save at  ./record/mask/05251729/ 
cuda:0 will be used in the training process !!! 
==================== pretext ==================== 
no pretext model in ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.010000 
2022-05-25 17:30:51- epoch: 1/512 - train loss : 4.129 - nce : 4.13 - coor(l/w) : 0.10/0.00 - mask(l/w) : 0.98/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.010000 
2022-05-25 17:31:54- epoch: 2/512 - train loss : 4.037 - nce : 4.04 - coor(l/w) : 0.12/0.00 - mask(l/w) : 1.18/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.010000 
2022-05-25 17:32:58- epoch: 3/512 - train loss : 3.981 - nce : 3.98 - coor(l/w) : 0.13/0.00 - mask(l/w) : 1.35/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.010000 
2022-05-25 17:33:58- epoch: 4/512 - train loss : 3.908 - nce : 3.91 - coor(l/w) : 0.13/0.00 - mask(l/w) : 1.66/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.010000 
2022-05-25 17:35:03- epoch: 5/512 - train loss : 3.871 - nce : 3.87 - coor(l/w) : 0.13/0.00 - mask(l/w) : 1.53/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.010000 
2022-05-25 17:36:05- epoch: 6/512 - train loss : 3.848 - nce : 3.85 - coor(l/w) : 0.12/0.00 - mask(l/w) : 1.39/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.010000 
2022-05-25 17:37:09- epoch: 7/512 - train loss : 3.785 - nce : 3.79 - coor(l/w) : 0.10/0.00 - mask(l/w) : 1.15/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.010000 
2022-05-25 17:38:14- epoch: 8/512 - train loss : 3.740 - nce : 3.74 - coor(l/w) : 0.09/0.00 - mask(l/w) : 1.05/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.010000 
2022-05-25 17:39:19- epoch: 9/512 - train loss : 3.638 - nce : 3.64 - coor(l/w) : 0.10/0.00 - mask(l/w) : 1.18/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.010000 
2022-05-25 17:40:25- epoch: 10/512 - train loss : 3.560 - nce : 3.56 - coor(l/w) : 0.12/0.00 - mask(l/w) : 1.14/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.010000 
2022-05-25 17:41:31- epoch: 11/512 - train loss : 3.509 - nce : 3.51 - coor(l/w) : 0.11/0.00 - mask(l/w) : 1.07/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.010000 
2022-05-25 17:42:39- epoch: 12/512 - train loss : 3.465 - nce : 3.46 - coor(l/w) : 0.09/0.00 - mask(l/w) : 0.98/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.010000 
2022-05-25 17:43:47- epoch: 13/512 - train loss : 3.450 - nce : 3.45 - coor(l/w) : 0.09/0.00 - mask(l/w) : 0.88/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009999 
2022-05-25 17:44:55- epoch: 14/512 - train loss : 3.428 - nce : 3.43 - coor(l/w) : 0.08/0.00 - mask(l/w) : 0.83/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009998 
2022-05-25 17:46:02- epoch: 15/512 - train loss : 3.413 - nce : 3.41 - coor(l/w) : 0.08/0.00 - mask(l/w) : 0.76/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009998 
2022-05-25 17:47:11- epoch: 16/512 - train loss : 3.400 - nce : 3.40 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.71/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009997 
2022-05-25 17:48:20- epoch: 17/512 - train loss : 3.404 - nce : 3.40 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.70/0.00  
current lr: 0.009995 
2022-05-25 17:49:29- epoch: 18/512 - train loss : 3.392 - nce : 3.39 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.66/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009994 
2022-05-25 17:50:39- epoch: 19/512 - train loss : 3.384 - nce : 3.38 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.63/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009992 
2022-05-25 17:51:49- epoch: 20/512 - train loss : 3.374 - nce : 3.37 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.60/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009991 
2022-05-25 17:53:00- epoch: 21/512 - train loss : 3.376 - nce : 3.38 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.56/0.00  
current lr: 0.009989 
2022-05-25 17:54:05- epoch: 22/512 - train loss : 3.369 - nce : 3.37 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.54/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009986 
2022-05-25 17:55:11- epoch: 23/512 - train loss : 3.364 - nce : 3.36 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.52/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009984 
2022-05-25 17:56:20- epoch: 24/512 - train loss : 3.367 - nce : 3.37 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.52/0.00  
current lr: 0.009982 
2022-05-25 17:57:26- epoch: 25/512 - train loss : 3.357 - nce : 3.36 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.51/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009979 
2022-05-25 17:58:33- epoch: 26/512 - train loss : 3.361 - nce : 3.36 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.48/0.00  
current lr: 0.009976 
2022-05-25 17:59:40- epoch: 27/512 - train loss : 3.352 - nce : 3.35 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.47/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009973 
2022-05-25 18:00:47- epoch: 28/512 - train loss : 3.350 - nce : 3.35 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.45/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009970 
2022-05-25 18:01:55- epoch: 29/512 - train loss : 3.346 - nce : 3.35 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.44/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009966 
2022-05-25 18:03:02- epoch: 30/512 - train loss : 3.350 - nce : 3.35 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.42/0.00  
current lr: 0.009962 
2022-05-25 18:04:10- epoch: 31/512 - train loss : 3.345 - nce : 3.35 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.40/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009959 
2022-05-25 18:05:19- epoch: 32/512 - train loss : 3.337 - nce : 3.34 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.39/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009955 
2022-05-25 18:06:29- epoch: 33/512 - train loss : 3.332 - nce : 3.33 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.38/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009950 
2022-05-25 18:07:37- epoch: 34/512 - train loss : 3.336 - nce : 3.34 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.37/0.00  
current lr: 0.009946 
2022-05-25 18:08:47- epoch: 35/512 - train loss : 3.332 - nce : 3.33 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.36/0.00  
current lr: 0.009941 
2022-05-25 18:09:58- epoch: 36/512 - train loss : 3.330 - nce : 3.33 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.35/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009937 
2022-05-25 18:11:09- epoch: 37/512 - train loss : 3.326 - nce : 3.33 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.34/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009932 
2022-05-25 18:12:20- epoch: 38/512 - train loss : 3.331 - nce : 3.33 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.34/0.00  
current lr: 0.009926 
2022-05-25 18:13:31- epoch: 39/512 - train loss : 3.323 - nce : 3.32 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.33/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009921 
2022-05-25 18:14:43- epoch: 40/512 - train loss : 3.318 - nce : 3.32 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.33/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009916 
2022-05-25 18:15:54- epoch: 41/512 - train loss : 3.319 - nce : 3.32 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.31/0.00  
current lr: 0.009910 
2022-05-25 18:17:05- epoch: 42/512 - train loss : 3.318 - nce : 3.32 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.32/0.00  
current lr: 0.009904 
2022-05-25 18:18:18- epoch: 43/512 - train loss : 3.317 - nce : 3.32 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.32/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009898 
2022-05-25 18:19:31- epoch: 44/512 - train loss : 3.315 - nce : 3.31 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.31/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009892 
2022-05-25 18:20:45- epoch: 45/512 - train loss : 3.309 - nce : 3.31 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.30/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009885 
2022-05-25 18:21:59- epoch: 46/512 - train loss : 3.311 - nce : 3.31 - coor(l/w) : 0.07/0.00 - mask(l/w) : 0.30/0.00  
current lr: 0.009879 
2022-05-25 18:23:14- epoch: 47/512 - train loss : 3.305 - nce : 3.30 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.31/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009872 
2022-05-25 18:24:31- epoch: 48/512 - train loss : 3.305 - nce : 3.30 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.30/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009865 
2022-05-25 18:25:49- epoch: 49/512 - train loss : 3.301 - nce : 3.30 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.29/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009858 
2022-05-25 18:27:08- epoch: 50/512 - train loss : 3.300 - nce : 3.30 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.28/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009850 
2022-05-25 18:28:26- epoch: 51/512 - train loss : 3.297 - nce : 3.30 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.28/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009843 
2022-05-25 18:29:46- epoch: 52/512 - train loss : 3.297 - nce : 3.30 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.28/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009835 
2022-05-25 18:31:05- epoch: 53/512 - train loss : 3.291 - nce : 3.29 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.28/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009827 
2022-05-25 18:32:25- epoch: 54/512 - train loss : 3.289 - nce : 3.29 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.29/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009819 
2022-05-25 18:33:44- epoch: 55/512 - train loss : 3.288 - nce : 3.29 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.29/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009811 
2022-05-25 18:35:05- epoch: 56/512 - train loss : 3.287 - nce : 3.29 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.28/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009802 
2022-05-25 18:36:25- epoch: 57/512 - train loss : 3.286 - nce : 3.29 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.28/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009794 
2022-05-25 18:37:46- epoch: 58/512 - train loss : 3.283 - nce : 3.28 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.28/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009785 
2022-05-25 18:39:07- epoch: 59/512 - train loss : 3.282 - nce : 3.28 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.28/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009776 
2022-05-25 18:40:29- epoch: 60/512 - train loss : 3.278 - nce : 3.28 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.27/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009767 
2022-05-25 18:41:50- epoch: 61/512 - train loss : 3.281 - nce : 3.28 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.26/0.00  
current lr: 0.009757 
2022-05-25 18:43:12- epoch: 62/512 - train loss : 3.280 - nce : 3.28 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.25/0.00  
current lr: 0.009748 
2022-05-25 18:44:34- epoch: 63/512 - train loss : 3.274 - nce : 3.27 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.24/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009738 
2022-05-25 18:45:58- epoch: 64/512 - train loss : 3.276 - nce : 3.28 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.24/0.00  
current lr: 0.009728 
2022-05-25 18:47:17- epoch: 65/512 - train loss : 3.274 - nce : 3.27 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.23/0.00  
current lr: 0.009718 
2022-05-25 18:48:39- epoch: 66/512 - train loss : 3.274 - nce : 3.27 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.21/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009708 
2022-05-25 18:50:04- epoch: 67/512 - train loss : 3.273 - nce : 3.27 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.22/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009697 
2022-05-25 18:51:27- epoch: 68/512 - train loss : 3.270 - nce : 3.27 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.22/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009687 
2022-05-25 18:52:51- epoch: 69/512 - train loss : 3.265 - nce : 3.27 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.21/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009676 
2022-05-25 18:54:16- epoch: 70/512 - train loss : 3.270 - nce : 3.27 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.20/0.00  
current lr: 0.009665 
2022-05-25 18:55:40- epoch: 71/512 - train loss : 3.268 - nce : 3.27 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.20/0.00  
current lr: 0.009654 
2022-05-25 18:57:05- epoch: 72/512 - train loss : 3.269 - nce : 3.27 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.19/0.00  
current lr: 0.009643 
2022-05-25 18:58:31- epoch: 73/512 - train loss : 3.264 - nce : 3.26 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.20/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009631 
2022-05-25 18:59:56- epoch: 74/512 - train loss : 3.267 - nce : 3.27 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.20/0.00  
current lr: 0.009619 
2022-05-25 19:01:22- epoch: 75/512 - train loss : 3.260 - nce : 3.26 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.18/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009608 
2022-05-25 19:02:48- epoch: 76/512 - train loss : 3.260 - nce : 3.26 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.17/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009596 
2022-05-25 19:04:14- epoch: 77/512 - train loss : 3.260 - nce : 3.26 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.17/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009583 
2022-05-25 19:05:41- epoch: 78/512 - train loss : 3.259 - nce : 3.26 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.17/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009571 
2022-05-25 19:07:08- epoch: 79/512 - train loss : 3.257 - nce : 3.26 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.16/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009559 
2022-05-25 19:08:36- epoch: 80/512 - train loss : 3.256 - nce : 3.26 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.16/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009546 
2022-05-25 19:10:03- epoch: 81/512 - train loss : 3.253 - nce : 3.25 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.16/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009533 
2022-05-25 19:11:31- epoch: 82/512 - train loss : 3.253 - nce : 3.25 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.16/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009520 
2022-05-25 19:12:59- epoch: 83/512 - train loss : 3.256 - nce : 3.26 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.15/0.00  
current lr: 0.009507 
2022-05-25 19:14:27- epoch: 84/512 - train loss : 3.251 - nce : 3.25 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.15/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009493 
2022-05-25 19:15:55- epoch: 85/512 - train loss : 3.251 - nce : 3.25 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.14/0.00  
current lr: 0.009480 
2022-05-25 19:17:20- epoch: 86/512 - train loss : 3.251 - nce : 3.25 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.14/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009466 
2022-05-25 19:18:45- epoch: 87/512 - train loss : 3.253 - nce : 3.25 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.14/0.00  
current lr: 0.009452 
2022-05-25 19:20:10- epoch: 88/512 - train loss : 3.250 - nce : 3.25 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.14/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009438 
2022-05-25 19:21:36- epoch: 89/512 - train loss : 3.249 - nce : 3.25 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.13/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009424 
2022-05-25 19:23:02- epoch: 90/512 - train loss : 3.246 - nce : 3.25 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.12/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009410 
2022-05-25 19:24:29- epoch: 91/512 - train loss : 3.245 - nce : 3.25 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.12/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009395 
2022-05-25 19:25:55- epoch: 92/512 - train loss : 3.244 - nce : 3.24 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.12/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009380 
2022-05-25 19:27:21- epoch: 93/512 - train loss : 3.245 - nce : 3.25 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.12/0.00  
current lr: 0.009365 
2022-05-25 19:28:50- epoch: 94/512 - train loss : 3.246 - nce : 3.25 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.11/0.00  
current lr: 0.009350 
2022-05-25 19:30:18- epoch: 95/512 - train loss : 3.243 - nce : 3.24 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.11/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009335 
2022-05-25 19:31:46- epoch: 96/512 - train loss : 3.243 - nce : 3.24 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.11/0.00  
current lr: 0.009320 
2022-05-25 19:33:14- epoch: 97/512 - train loss : 3.242 - nce : 3.24 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.10/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009304 
2022-05-25 19:34:42- epoch: 98/512 - train loss : 3.239 - nce : 3.24 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.10/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009289 
2022-05-25 19:36:11- epoch: 99/512 - train loss : 3.242 - nce : 3.24 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.10/0.00  
current lr: 0.009273 
2022-05-25 19:37:40- epoch: 100/512 - train loss : 3.241 - nce : 3.24 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.10/0.00  
current lr: 0.009257 
2022-05-25 19:39:10- epoch: 101/512 - train loss : 3.239 - nce : 3.24 - coor(l/w) : 0.06/0.00 - mask(l/w) : 0.09/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009241 
2022-05-25 19:40:40- epoch: 102/512 - train loss : 3.240 - nce : 3.24 - coor(l/w) : 0.05/0.00 - mask(l/w) : 0.04/0.00  
current lr: 0.009224 
2022-05-25 19:42:10- epoch: 103/512 - train loss : 3.238 - nce : 3.24 - coor(l/w) : 0.05/0.00 - mask(l/w) : 0.02/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009208 
2022-05-25 19:43:41- epoch: 104/512 - train loss : 3.238 - nce : 3.24 - coor(l/w) : 0.04/0.00 - mask(l/w) : 0.02/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009191 
2022-05-25 19:45:12- epoch: 105/512 - train loss : 3.236 - nce : 3.24 - coor(l/w) : 0.04/0.00 - mask(l/w) : 0.02/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009174 
2022-05-25 19:46:44- epoch: 106/512 - train loss : 3.235 - nce : 3.24 - coor(l/w) : 0.04/0.00 - mask(l/w) : 0.01/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009157 
2022-05-25 19:48:16- epoch: 107/512 - train loss : 3.234 - nce : 3.23 - coor(l/w) : 0.04/0.00 - mask(l/w) : 0.01/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009140 
2022-05-25 19:49:50- epoch: 108/512 - train loss : 3.234 - nce : 3.23 - coor(l/w) : 0.04/0.00 - mask(l/w) : 0.01/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009123 
2022-05-25 19:51:22- epoch: 109/512 - train loss : 3.233 - nce : 3.23 - coor(l/w) : 0.04/0.00 - mask(l/w) : 0.01/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009106 
2022-05-25 19:52:55- epoch: 110/512 - train loss : 3.233 - nce : 3.23 - coor(l/w) : 0.04/0.00 - mask(l/w) : 0.01/0.00  
current lr: 0.009088 
2022-05-25 19:54:29- epoch: 111/512 - train loss : 3.231 - nce : 3.23 - coor(l/w) : 0.04/0.00 - mask(l/w) : 0.01/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009070 
2022-05-25 19:56:05- epoch: 112/512 - train loss : 3.231 - nce : 3.23 - coor(l/w) : 0.03/0.00 - mask(l/w) : 0.01/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009052 
2022-05-25 19:57:40- epoch: 113/512 - train loss : 3.231 - nce : 3.23 - coor(l/w) : 0.03/0.00 - mask(l/w) : 0.01/0.00  
current lr: 0.009034 
2022-05-25 19:59:14- epoch: 114/512 - train loss : 3.230 - nce : 3.23 - coor(l/w) : 0.03/0.00 - mask(l/w) : 0.01/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.009016 
2022-05-25 20:00:49- epoch: 115/512 - train loss : 3.230 - nce : 3.23 - coor(l/w) : 0.03/0.00 - mask(l/w) : 0.01/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.008998 
2022-05-25 20:02:24- epoch: 116/512 - train loss : 3.227 - nce : 3.23 - coor(l/w) : 0.03/0.00 - mask(l/w) : 0.01/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.008979 
2022-05-25 20:04:00- epoch: 117/512 - train loss : 3.228 - nce : 3.23 - coor(l/w) : 0.03/0.00 - mask(l/w) : 0.01/0.00  
current lr: 0.008961 
2022-05-25 20:05:35- epoch: 118/512 - train loss : 3.227 - nce : 3.23 - coor(l/w) : 0.03/0.00 - mask(l/w) : 0.01/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.008942 
2022-05-25 20:07:12- epoch: 119/512 - train loss : 3.228 - nce : 3.23 - coor(l/w) : 0.03/0.00 - mask(l/w) : 0.01/0.00  
current lr: 0.008923 
2022-05-25 20:08:48- epoch: 120/512 - train loss : 3.226 - nce : 3.23 - coor(l/w) : 0.03/0.00 - mask(l/w) : 0.01/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.008904 
2022-05-25 20:10:25- epoch: 121/512 - train loss : 3.226 - nce : 3.23 - coor(l/w) : 0.03/0.00 - mask(l/w) : 0.01/0.00  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.008884 
2022-05-25 20:12:03- epoch: 122/512 - train loss : 3.226 - nce : 3.23 - coor(l/w) : 0.03/0.01 - mask(l/w) : 0.01/0.01  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.008865 
2022-05-25 20:13:43- epoch: 123/512 - train loss : 3.225 - nce : 3.22 - coor(l/w) : 0.03/0.01 - mask(l/w) : 0.01/0.01  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.008846 
2022-05-25 20:15:23- epoch: 124/512 - train loss : 3.225 - nce : 3.23 - coor(l/w) : 0.03/0.01 - mask(l/w) : 0.01/0.01  
current lr: 0.008826 
2022-05-25 20:17:02- epoch: 125/512 - train loss : 3.225 - nce : 3.22 - coor(l/w) : 0.03/0.01 - mask(l/w) : 0.01/0.01  
current lr: 0.008806 
2022-05-25 20:18:43- epoch: 126/512 - train loss : 3.223 - nce : 3.22 - coor(l/w) : 0.03/0.01 - mask(l/w) : 0.01/0.01  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.008786 
2022-05-25 20:20:24- epoch: 127/512 - train loss : 3.223 - nce : 3.22 - coor(l/w) : 0.03/0.01 - mask(l/w) : 0.01/0.01  
current lr: 0.008766 
2022-05-25 20:22:06- epoch: 128/512 - train loss : 3.224 - nce : 3.22 - coor(l/w) : 0.03/0.01 - mask(l/w) : 0.01/0.01  
current lr: 0.008746 
2022-05-25 20:23:45- epoch: 129/512 - train loss : 3.223 - nce : 3.22 - coor(l/w) : 0.03/0.01 - mask(l/w) : 0.01/0.01  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.008725 
2022-05-25 20:25:26- epoch: 130/512 - train loss : 3.223 - nce : 3.22 - coor(l/w) : 0.03/0.01 - mask(l/w) : 0.01/0.01  
current lr: 0.008705 
2022-05-25 20:27:07- epoch: 131/512 - train loss : 3.224 - nce : 3.22 - coor(l/w) : 0.03/0.01 - mask(l/w) : 0.01/0.01  
current lr: 0.008684 
2022-05-25 20:28:50- epoch: 132/512 - train loss : 3.222 - nce : 3.22 - coor(l/w) : 0.03/0.01 - mask(l/w) : 0.01/0.01  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.008663 
2022-05-25 20:30:33- epoch: 133/512 - train loss : 3.222 - nce : 3.22 - coor(l/w) : 0.03/0.01 - mask(l/w) : 0.00/0.01  
current lr: 0.008642 
2022-05-25 20:32:16- epoch: 134/512 - train loss : 3.220 - nce : 3.22 - coor(l/w) : 0.03/0.01 - mask(l/w) : 0.00/0.01  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.008621 
2022-05-25 20:34:00- epoch: 135/512 - train loss : 3.221 - nce : 3.22 - coor(l/w) : 0.03/0.01 - mask(l/w) : 0.00/0.01  
current lr: 0.008600 
2022-05-25 20:35:40- epoch: 136/512 - train loss : 3.220 - nce : 3.22 - coor(l/w) : 0.03/0.01 - mask(l/w) : 0.00/0.01  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.008579 
2022-05-25 20:37:23- epoch: 137/512 - train loss : 3.219 - nce : 3.22 - coor(l/w) : 0.03/0.01 - mask(l/w) : 0.00/0.01  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.008557 
2022-05-25 20:39:08- epoch: 138/512 - train loss : 3.218 - nce : 3.22 - coor(l/w) : 0.03/0.01 - mask(l/w) : 0.00/0.01  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.008536 
2022-05-25 20:40:49- epoch: 139/512 - train loss : 3.220 - nce : 3.22 - coor(l/w) : 0.03/0.01 - mask(l/w) : 0.00/0.01  
current lr: 0.008514 
2022-05-25 20:42:31- epoch: 140/512 - train loss : 3.217 - nce : 3.22 - coor(l/w) : 0.03/0.01 - mask(l/w) : 0.00/0.01  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.008492 
2022-05-25 20:44:15- epoch: 141/512 - train loss : 3.218 - nce : 3.22 - coor(l/w) : 0.03/0.01 - mask(l/w) : 0.00/0.01  
current lr: 0.008470 
2022-05-25 20:45:59- epoch: 142/512 - train loss : 3.219 - nce : 3.22 - coor(l/w) : 0.03/0.01 - mask(l/w) : 0.00/0.01  
current lr: 0.008448 
2022-05-25 20:47:40- epoch: 143/512 - train loss : 3.218 - nce : 3.22 - coor(l/w) : 0.02/0.01 - mask(l/w) : 0.00/0.01  
current lr: 0.008425 
2022-05-25 20:49:22- epoch: 144/512 - train loss : 3.217 - nce : 3.22 - coor(l/w) : 0.02/0.01 - mask(l/w) : 0.00/0.01  
current lr: 0.008403 
2022-05-25 20:51:05- epoch: 145/512 - train loss : 3.217 - nce : 3.22 - coor(l/w) : 0.02/0.01 - mask(l/w) : 0.00/0.01  
current lr: 0.008380 
2022-05-25 20:52:48- epoch: 146/512 - train loss : 3.217 - nce : 3.22 - coor(l/w) : 0.02/0.01 - mask(l/w) : 0.00/0.01  
current lr: 0.008358 
2022-05-25 20:54:32- epoch: 147/512 - train loss : 3.215 - nce : 3.21 - coor(l/w) : 0.02/0.01 - mask(l/w) : 0.00/0.01  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.008335 
2022-05-25 20:56:17- epoch: 148/512 - train loss : 3.215 - nce : 3.22 - coor(l/w) : 0.02/0.01 - mask(l/w) : 0.00/0.01  
current lr: 0.008312 
2022-05-25 20:58:01- epoch: 149/512 - train loss : 3.215 - nce : 3.22 - coor(l/w) : 0.02/0.01 - mask(l/w) : 0.00/0.01  
current lr: 0.008289 
2022-05-25 20:59:45- epoch: 150/512 - train loss : 3.216 - nce : 3.22 - coor(l/w) : 0.02/0.01 - mask(l/w) : 0.00/0.01  
current lr: 0.008266 
2022-05-25 21:01:32- epoch: 151/512 - train loss : 3.215 - nce : 3.21 - coor(l/w) : 0.02/0.01 - mask(l/w) : 0.00/0.01  
current lr: 0.008243 
2022-05-25 21:03:19- epoch: 152/512 - train loss : 3.215 - nce : 3.21 - coor(l/w) : 0.02/0.01 - mask(l/w) : 0.00/0.01  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.008219 
2022-05-25 21:05:08- epoch: 153/512 - train loss : 3.214 - nce : 3.21 - coor(l/w) : 0.02/0.01 - mask(l/w) : 0.00/0.01  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.008196 
2022-05-25 21:06:56- epoch: 154/512 - train loss : 3.215 - nce : 3.21 - coor(l/w) : 0.02/0.01 - mask(l/w) : 0.00/0.01  
current lr: 0.008172 
2022-05-25 21:08:46- epoch: 155/512 - train loss : 3.215 - nce : 3.21 - coor(l/w) : 0.02/0.01 - mask(l/w) : 0.00/0.01  
current lr: 0.008148 
2022-05-25 21:10:34- epoch: 156/512 - train loss : 3.212 - nce : 3.21 - coor(l/w) : 0.02/0.01 - mask(l/w) : 0.00/0.01  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.008124 
2022-05-25 21:12:24- epoch: 157/512 - train loss : 3.214 - nce : 3.21 - coor(l/w) : 0.02/0.01 - mask(l/w) : 0.00/0.01  
current lr: 0.008100 
2022-05-25 21:14:15- epoch: 158/512 - train loss : 3.212 - nce : 3.21 - coor(l/w) : 0.02/0.01 - mask(l/w) : 0.00/0.01  
current lr: 0.008076 
2022-05-25 21:16:04- epoch: 159/512 - train loss : 3.211 - nce : 3.21 - coor(l/w) : 0.02/0.01 - mask(l/w) : 0.00/0.01  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.008052 
2022-05-25 21:17:55- epoch: 160/512 - train loss : 3.211 - nce : 3.21 - coor(l/w) : 0.02/0.01 - mask(l/w) : 0.00/0.01  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.008028 
2022-05-25 21:19:46- epoch: 161/512 - train loss : 3.212 - nce : 3.21 - coor(l/w) : 0.02/0.01 - mask(l/w) : 0.00/0.01  
current lr: 0.008003 
2022-05-25 21:21:38- epoch: 162/512 - train loss : 3.211 - nce : 3.21 - coor(l/w) : 0.02/0.01 - mask(l/w) : 0.00/0.01  
current lr: 0.007978 
2022-05-25 21:23:29- epoch: 163/512 - train loss : 3.212 - nce : 3.21 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
current lr: 0.007954 
2022-05-25 21:25:21- epoch: 164/512 - train loss : 3.211 - nce : 3.21 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.007929 
2022-05-25 21:27:14- epoch: 165/512 - train loss : 3.211 - nce : 3.21 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
current lr: 0.007904 
2022-05-25 21:29:03- epoch: 166/512 - train loss : 3.210 - nce : 3.21 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.007879 
2022-05-25 21:30:52- epoch: 167/512 - train loss : 3.209 - nce : 3.21 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.007854 
2022-05-25 21:32:43- epoch: 168/512 - train loss : 3.209 - nce : 3.21 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.007829 
2022-05-25 21:34:33- epoch: 169/512 - train loss : 3.209 - nce : 3.21 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
current lr: 0.007803 
2022-05-25 21:36:22- epoch: 170/512 - train loss : 3.209 - nce : 3.21 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.007778 
2022-05-25 21:38:13- epoch: 171/512 - train loss : 3.209 - nce : 3.21 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
current lr: 0.007752 
2022-05-25 21:40:04- epoch: 172/512 - train loss : 3.207 - nce : 3.21 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.007727 
2022-05-25 21:41:56- epoch: 173/512 - train loss : 3.207 - nce : 3.21 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.007701 
2022-05-25 21:43:49- epoch: 174/512 - train loss : 3.210 - nce : 3.21 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
current lr: 0.007675 
2022-05-25 21:45:41- epoch: 175/512 - train loss : 3.206 - nce : 3.21 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.007649 
2022-05-25 21:47:35- epoch: 176/512 - train loss : 3.209 - nce : 3.21 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
current lr: 0.007623 
2022-05-25 21:49:29- epoch: 177/512 - train loss : 3.207 - nce : 3.21 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
current lr: 0.007597 
2022-05-25 21:51:23- epoch: 178/512 - train loss : 3.205 - nce : 3.20 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.007571 
2022-05-25 21:53:17- epoch: 179/512 - train loss : 3.206 - nce : 3.21 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
current lr: 0.007544 
2022-05-25 21:55:12- epoch: 180/512 - train loss : 3.207 - nce : 3.21 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
current lr: 0.007518 
2022-05-25 21:57:07- epoch: 181/512 - train loss : 3.206 - nce : 3.21 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
current lr: 0.007491 
2022-05-25 21:59:02- epoch: 182/512 - train loss : 3.204 - nce : 3.20 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.007464 
2022-05-25 22:00:57- epoch: 183/512 - train loss : 3.204 - nce : 3.20 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.007438 
2022-05-25 22:02:53- epoch: 184/512 - train loss : 3.204 - nce : 3.20 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
current lr: 0.007411 
2022-05-25 22:04:51- epoch: 185/512 - train loss : 3.205 - nce : 3.20 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
current lr: 0.007384 
2022-05-25 22:06:48- epoch: 186/512 - train loss : 3.205 - nce : 3.20 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
current lr: 0.007357 
2022-05-25 22:08:47- epoch: 187/512 - train loss : 3.204 - nce : 3.20 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.007330 
2022-05-25 22:10:46- epoch: 188/512 - train loss : 3.203 - nce : 3.20 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.007303 
2022-05-25 22:12:46- epoch: 189/512 - train loss : 3.203 - nce : 3.20 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
current lr: 0.007275 
2022-05-25 22:14:46- epoch: 190/512 - train loss : 3.205 - nce : 3.20 - coor(l/w) : 0.02/0.02 - mask(l/w) : 0.00/0.02  
current lr: 0.007248 
2022-05-25 22:16:46- epoch: 191/512 - train loss : 3.202 - nce : 3.20 - coor(l/w) : 0.01/0.02 - mask(l/w) : 0.00/0.02  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.007221 
2022-05-25 22:18:47- epoch: 192/512 - train loss : 3.205 - nce : 3.20 - coor(l/w) : 0.01/0.02 - mask(l/w) : 0.00/0.02  
current lr: 0.007193 
2022-05-25 22:20:48- epoch: 193/512 - train loss : 3.202 - nce : 3.20 - coor(l/w) : 0.01/0.02 - mask(l/w) : 0.00/0.02  
current lr: 0.007165 
2022-05-25 22:22:49- epoch: 194/512 - train loss : 3.203 - nce : 3.20 - coor(l/w) : 0.01/0.02 - mask(l/w) : 0.00/0.02  
current lr: 0.007138 
2022-05-25 22:24:50- epoch: 195/512 - train loss : 3.202 - nce : 3.20 - coor(l/w) : 0.01/0.02 - mask(l/w) : 0.00/0.02  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.007110 
2022-05-25 22:26:52- epoch: 196/512 - train loss : 3.200 - nce : 3.20 - coor(l/w) : 0.01/0.02 - mask(l/w) : 0.00/0.02  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.007082 
2022-05-25 22:28:55- epoch: 197/512 - train loss : 3.202 - nce : 3.20 - coor(l/w) : 0.01/0.02 - mask(l/w) : 0.00/0.02  
current lr: 0.007054 
2022-05-25 22:30:53- epoch: 198/512 - train loss : 3.203 - nce : 3.20 - coor(l/w) : 0.01/0.02 - mask(l/w) : 0.00/0.02  
current lr: 0.007026 
2022-05-25 22:32:53- epoch: 199/512 - train loss : 3.202 - nce : 3.20 - coor(l/w) : 0.01/0.02 - mask(l/w) : 0.00/0.02  
current lr: 0.006998 
2022-05-25 22:34:52- epoch: 200/512 - train loss : 3.200 - nce : 3.20 - coor(l/w) : 0.01/0.02 - mask(l/w) : 0.00/0.02  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.006970 
2022-05-25 22:36:52- epoch: 201/512 - train loss : 3.201 - nce : 3.20 - coor(l/w) : 0.01/0.02 - mask(l/w) : 0.00/0.02  
current lr: 0.006942 
2022-05-25 22:38:52- epoch: 202/512 - train loss : 3.201 - nce : 3.20 - coor(l/w) : 0.01/0.02 - mask(l/w) : 0.00/0.02  
current lr: 0.006913 
2022-05-25 22:40:52- epoch: 203/512 - train loss : 3.200 - nce : 3.20 - coor(l/w) : 0.01/0.02 - mask(l/w) : 0.00/0.02  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.006885 
2022-05-25 22:42:55- epoch: 204/512 - train loss : 3.201 - nce : 3.20 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.006857 
2022-05-25 22:44:56- epoch: 205/512 - train loss : 3.199 - nce : 3.20 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.006828 
2022-05-25 22:46:59- epoch: 206/512 - train loss : 3.199 - nce : 3.20 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.006799 
2022-05-25 22:49:02- epoch: 207/512 - train loss : 3.200 - nce : 3.20 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.006771 
2022-05-25 22:51:04- epoch: 208/512 - train loss : 3.199 - nce : 3.20 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.006742 
2022-05-25 22:53:07- epoch: 209/512 - train loss : 3.198 - nce : 3.20 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.006713 
2022-05-25 22:55:11- epoch: 210/512 - train loss : 3.198 - nce : 3.20 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.006684 
2022-05-25 22:57:15- epoch: 211/512 - train loss : 3.199 - nce : 3.20 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.006656 
2022-05-25 22:59:18- epoch: 212/512 - train loss : 3.197 - nce : 3.20 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.006627 
2022-05-25 23:01:22- epoch: 213/512 - train loss : 3.198 - nce : 3.20 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.006598 
2022-05-25 23:03:27- epoch: 214/512 - train loss : 3.198 - nce : 3.20 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.006568 
2022-05-25 23:05:33- epoch: 215/512 - train loss : 3.198 - nce : 3.20 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.006539 
2022-05-25 23:07:38- epoch: 216/512 - train loss : 3.197 - nce : 3.20 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.006510 
2022-05-25 23:09:43- epoch: 217/512 - train loss : 3.197 - nce : 3.20 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.006481 
2022-05-25 23:11:51- epoch: 218/512 - train loss : 3.196 - nce : 3.20 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.006451 
2022-05-25 23:14:00- epoch: 219/512 - train loss : 3.195 - nce : 3.19 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.006422 
2022-05-25 23:16:09- epoch: 220/512 - train loss : 3.197 - nce : 3.20 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.006393 
2022-05-25 23:18:15- epoch: 221/512 - train loss : 3.195 - nce : 3.19 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.006363 
2022-05-25 23:20:24- epoch: 222/512 - train loss : 3.196 - nce : 3.20 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.006334 
2022-05-25 23:22:32- epoch: 223/512 - train loss : 3.195 - nce : 3.20 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.006304 
2022-05-25 23:24:41- epoch: 224/512 - train loss : 3.195 - nce : 3.19 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.006274 
2022-05-25 23:26:51- epoch: 225/512 - train loss : 3.196 - nce : 3.20 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.006245 
2022-05-25 23:28:58- epoch: 226/512 - train loss : 3.197 - nce : 3.20 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.006215 
2022-05-25 23:31:04- epoch: 227/512 - train loss : 3.196 - nce : 3.20 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.006185 
2022-05-25 23:33:11- epoch: 228/512 - train loss : 3.194 - nce : 3.19 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.006155 
2022-05-25 23:35:19- epoch: 229/512 - train loss : 3.194 - nce : 3.19 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.006125 
2022-05-25 23:37:30- epoch: 230/512 - train loss : 3.196 - nce : 3.20 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.006096 
2022-05-25 23:39:38- epoch: 231/512 - train loss : 3.196 - nce : 3.20 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.006066 
2022-05-25 23:41:48- epoch: 232/512 - train loss : 3.194 - nce : 3.19 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.006036 
2022-05-25 23:43:56- epoch: 233/512 - train loss : 3.194 - nce : 3.19 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.006006 
2022-05-25 23:46:07- epoch: 234/512 - train loss : 3.195 - nce : 3.19 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.005975 
2022-05-25 23:48:17- epoch: 235/512 - train loss : 3.192 - nce : 3.19 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.005945 
2022-05-25 23:50:27- epoch: 236/512 - train loss : 3.192 - nce : 3.19 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.005915 
2022-05-25 23:52:37- epoch: 237/512 - train loss : 3.193 - nce : 3.19 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.005885 
2022-05-25 23:54:49- epoch: 238/512 - train loss : 3.192 - nce : 3.19 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.005855 
2022-05-25 23:57:01- epoch: 239/512 - train loss : 3.194 - nce : 3.19 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.005825 
2022-05-25 23:59:14- epoch: 240/512 - train loss : 3.192 - nce : 3.19 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.005794 
2022-05-26 00:01:26- epoch: 241/512 - train loss : 3.192 - nce : 3.19 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.005764 
2022-05-26 00:03:39- epoch: 242/512 - train loss : 3.192 - nce : 3.19 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.005734 
2022-05-26 00:05:52- epoch: 243/512 - train loss : 3.192 - nce : 3.19 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.005703 
2022-05-26 00:08:06- epoch: 244/512 - train loss : 3.192 - nce : 3.19 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
current lr: 0.005673 
2022-05-26 00:10:21- epoch: 245/512 - train loss : 3.191 - nce : 3.19 - coor(l/w) : 0.01/0.03 - mask(l/w) : 0.00/0.03  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.005642 
2022-05-26 00:12:38- epoch: 246/512 - train loss : 3.191 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.005612 
2022-05-26 00:14:54- epoch: 247/512 - train loss : 3.191 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.005582 
2022-05-26 00:17:10- epoch: 248/512 - train loss : 3.190 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.005551 
2022-05-26 00:19:28- epoch: 249/512 - train loss : 3.190 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.005521 
2022-05-26 00:21:46- epoch: 250/512 - train loss : 3.189 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.005490 
2022-05-26 00:24:04- epoch: 251/512 - train loss : 3.190 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.005460 
2022-05-26 00:26:23- epoch: 252/512 - train loss : 3.188 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.005429 
2022-05-26 00:28:42- epoch: 253/512 - train loss : 3.188 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.005398 
2022-05-26 00:31:00- epoch: 254/512 - train loss : 3.189 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.005368 
2022-05-26 00:33:19- epoch: 255/512 - train loss : 3.189 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.005337 
2022-05-26 00:35:39- epoch: 256/512 - train loss : 3.188 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.005307 
2022-05-26 00:38:00- epoch: 257/512 - train loss : 3.190 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.005276 
2022-05-26 00:40:21- epoch: 258/512 - train loss : 3.190 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.005245 
2022-05-26 00:42:41- epoch: 259/512 - train loss : 3.189 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.005215 
2022-05-26 00:45:03- epoch: 260/512 - train loss : 3.189 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.005184 
2022-05-26 00:47:25- epoch: 261/512 - train loss : 3.188 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.005153 
2022-05-26 00:49:46- epoch: 262/512 - train loss : 3.189 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.005123 
2022-05-26 00:52:08- epoch: 263/512 - train loss : 3.188 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.005092 
2022-05-26 00:54:29- epoch: 264/512 - train loss : 3.188 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.005061 
2022-05-26 00:56:51- epoch: 265/512 - train loss : 3.188 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.005031 
2022-05-26 00:59:12- epoch: 266/512 - train loss : 3.188 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.005000 
2022-05-26 01:01:35- epoch: 267/512 - train loss : 3.186 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.004969 
2022-05-26 01:03:58- epoch: 268/512 - train loss : 3.187 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.004939 
2022-05-26 01:06:20- epoch: 269/512 - train loss : 3.187 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.004908 
2022-05-26 01:08:44- epoch: 270/512 - train loss : 3.187 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.004877 
2022-05-26 01:11:07- epoch: 271/512 - train loss : 3.187 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.004847 
2022-05-26 01:13:28- epoch: 272/512 - train loss : 3.186 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.004816 
2022-05-26 01:15:53- epoch: 273/512 - train loss : 3.185 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.004785 
2022-05-26 01:18:18- epoch: 274/512 - train loss : 3.187 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.004755 
2022-05-26 01:20:44- epoch: 275/512 - train loss : 3.188 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.004724 
2022-05-26 01:23:09- epoch: 276/512 - train loss : 3.186 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.004693 
2022-05-26 01:25:35- epoch: 277/512 - train loss : 3.187 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.004663 
2022-05-26 01:27:57- epoch: 278/512 - train loss : 3.185 - nce : 3.18 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.004632 
2022-05-26 01:30:23- epoch: 279/512 - train loss : 3.185 - nce : 3.18 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.004602 
2022-05-26 01:32:50- epoch: 280/512 - train loss : 3.186 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.004571 
2022-05-26 01:35:18- epoch: 281/512 - train loss : 3.186 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.004540 
2022-05-26 01:37:45- epoch: 282/512 - train loss : 3.184 - nce : 3.18 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.004510 
2022-05-26 01:40:13- epoch: 283/512 - train loss : 3.185 - nce : 3.18 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.004479 
2022-05-26 01:42:37- epoch: 284/512 - train loss : 3.185 - nce : 3.18 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.004449 
2022-05-26 01:45:00- epoch: 285/512 - train loss : 3.184 - nce : 3.18 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.004418 
2022-05-26 01:47:24- epoch: 286/512 - train loss : 3.185 - nce : 3.19 - coor(l/w) : 0.01/0.04 - mask(l/w) : 0.00/0.04  
current lr: 0.004388 
2022-05-26 01:49:47- epoch: 287/512 - train loss : 3.183 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.004358 
2022-05-26 01:52:11- epoch: 288/512 - train loss : 3.183 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.004327 
2022-05-26 01:54:36- epoch: 289/512 - train loss : 3.184 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.004297 
2022-05-26 01:57:01- epoch: 290/512 - train loss : 3.184 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.004266 
2022-05-26 01:59:26- epoch: 291/512 - train loss : 3.183 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.004236 
2022-05-26 02:01:51- epoch: 292/512 - train loss : 3.184 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.004206 
2022-05-26 02:04:17- epoch: 293/512 - train loss : 3.184 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.004175 
2022-05-26 02:06:42- epoch: 294/512 - train loss : 3.182 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.004145 
2022-05-26 02:09:08- epoch: 295/512 - train loss : 3.183 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.004115 
2022-05-26 02:11:34- epoch: 296/512 - train loss : 3.183 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.004085 
2022-05-26 02:14:00- epoch: 297/512 - train loss : 3.182 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.004055 
2022-05-26 02:16:27- epoch: 298/512 - train loss : 3.182 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.004025 
2022-05-26 02:18:54- epoch: 299/512 - train loss : 3.182 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.003994 
2022-05-26 02:21:22- epoch: 300/512 - train loss : 3.181 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.003964 
2022-05-26 02:23:51- epoch: 301/512 - train loss : 3.182 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.003934 
2022-05-26 02:26:19- epoch: 302/512 - train loss : 3.181 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.003904 
2022-05-26 02:28:47- epoch: 303/512 - train loss : 3.182 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.003875 
2022-05-26 02:31:16- epoch: 304/512 - train loss : 3.181 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.003845 
2022-05-26 02:33:45- epoch: 305/512 - train loss : 3.182 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.003815 
2022-05-26 02:36:14- epoch: 306/512 - train loss : 3.181 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.003785 
2022-05-26 02:38:43- epoch: 307/512 - train loss : 3.182 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.003755 
2022-05-26 02:41:13- epoch: 308/512 - train loss : 3.181 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.003726 
2022-05-26 02:43:44- epoch: 309/512 - train loss : 3.182 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.003696 
2022-05-26 02:46:14- epoch: 310/512 - train loss : 3.181 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.003666 
2022-05-26 02:48:45- epoch: 311/512 - train loss : 3.181 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.003637 
2022-05-26 02:51:17- epoch: 312/512 - train loss : 3.181 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.003607 
2022-05-26 02:53:48- epoch: 313/512 - train loss : 3.181 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.003578 
2022-05-26 02:56:20- epoch: 314/512 - train loss : 3.180 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.003549 
2022-05-26 02:58:52- epoch: 315/512 - train loss : 3.180 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.003519 
2022-05-26 03:01:25- epoch: 316/512 - train loss : 3.180 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.003490 
2022-05-26 03:03:57- epoch: 317/512 - train loss : 3.180 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.003461 
2022-05-26 03:06:31- epoch: 318/512 - train loss : 3.179 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.003432 
2022-05-26 03:09:05- epoch: 319/512 - train loss : 3.179 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.003402 
2022-05-26 03:11:40- epoch: 320/512 - train loss : 3.179 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.003373 
2022-05-26 03:14:15- epoch: 321/512 - train loss : 3.178 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.003344 
2022-05-26 03:16:52- epoch: 322/512 - train loss : 3.179 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.003316 
2022-05-26 03:19:30- epoch: 323/512 - train loss : 3.180 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.003287 
2022-05-26 03:22:09- epoch: 324/512 - train loss : 3.179 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
current lr: 0.003258 
2022-05-26 03:24:45- epoch: 325/512 - train loss : 3.178 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.003229 
2022-05-26 03:27:24- epoch: 326/512 - train loss : 3.178 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.003201 
2022-05-26 03:30:05- epoch: 327/512 - train loss : 3.177 - nce : 3.18 - coor(l/w) : 0.01/0.05 - mask(l/w) : 0.00/0.05  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.003172 
2022-05-26 03:32:46- epoch: 328/512 - train loss : 3.178 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.003143 
2022-05-26 03:35:26- epoch: 329/512 - train loss : 3.179 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.003115 
2022-05-26 03:38:07- epoch: 330/512 - train loss : 3.181 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.003087 
2022-05-26 03:40:48- epoch: 331/512 - train loss : 3.179 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.003058 
2022-05-26 03:43:29- epoch: 332/512 - train loss : 3.177 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.003030 
2022-05-26 03:46:11- epoch: 333/512 - train loss : 3.179 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.003002 
2022-05-26 03:48:54- epoch: 334/512 - train loss : 3.177 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.002974 
2022-05-26 03:51:37- epoch: 335/512 - train loss : 3.178 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.002946 
2022-05-26 03:54:17- epoch: 336/512 - train loss : 3.177 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.002918 
2022-05-26 03:56:59- epoch: 337/512 - train loss : 3.178 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.002890 
2022-05-26 03:59:40- epoch: 338/512 - train loss : 3.178 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.002862 
2022-05-26 04:02:21- epoch: 339/512 - train loss : 3.177 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.002835 
2022-05-26 04:05:04- epoch: 340/512 - train loss : 3.177 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.002807 
2022-05-26 04:07:45- epoch: 341/512 - train loss : 3.176 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.002779 
2022-05-26 04:10:28- epoch: 342/512 - train loss : 3.176 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.002752 
2022-05-26 04:13:13- epoch: 343/512 - train loss : 3.176 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.002725 
2022-05-26 04:15:56- epoch: 344/512 - train loss : 3.176 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.002697 
2022-05-26 04:18:39- epoch: 345/512 - train loss : 3.177 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.002670 
2022-05-26 04:21:21- epoch: 346/512 - train loss : 3.176 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.002643 
2022-05-26 04:24:06- epoch: 347/512 - train loss : 3.176 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.002616 
2022-05-26 04:26:52- epoch: 348/512 - train loss : 3.175 - nce : 3.17 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.002589 
2022-05-26 04:29:38- epoch: 349/512 - train loss : 3.177 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.002562 
2022-05-26 04:32:25- epoch: 350/512 - train loss : 3.176 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.002536 
2022-05-26 04:35:12- epoch: 351/512 - train loss : 3.175 - nce : 3.17 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.002509 
2022-05-26 04:37:59- epoch: 352/512 - train loss : 3.176 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.002482 
2022-05-26 04:40:47- epoch: 353/512 - train loss : 3.176 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.002456 
2022-05-26 04:43:35- epoch: 354/512 - train loss : 3.175 - nce : 3.17 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.002429 
2022-05-26 04:46:23- epoch: 355/512 - train loss : 3.175 - nce : 3.17 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.002403 
2022-05-26 04:49:12- epoch: 356/512 - train loss : 3.176 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.002377 
2022-05-26 04:52:01- epoch: 357/512 - train loss : 3.174 - nce : 3.17 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.002351 
2022-05-26 04:54:52- epoch: 358/512 - train loss : 3.175 - nce : 3.17 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.002325 
2022-05-26 04:57:38- epoch: 359/512 - train loss : 3.175 - nce : 3.18 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.002299 
2022-05-26 05:00:25- epoch: 360/512 - train loss : 3.175 - nce : 3.17 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.002273 
2022-05-26 05:03:15- epoch: 361/512 - train loss : 3.175 - nce : 3.17 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.002248 
2022-05-26 05:06:03- epoch: 362/512 - train loss : 3.174 - nce : 3.17 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.002222 
2022-05-26 05:08:53- epoch: 363/512 - train loss : 3.174 - nce : 3.17 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.002197 
2022-05-26 05:11:43- epoch: 364/512 - train loss : 3.174 - nce : 3.17 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.002171 
2022-05-26 05:14:33- epoch: 365/512 - train loss : 3.173 - nce : 3.17 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.002146 
2022-05-26 05:17:24- epoch: 366/512 - train loss : 3.173 - nce : 3.17 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.002121 
2022-05-26 05:20:15- epoch: 367/512 - train loss : 3.174 - nce : 3.17 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.002096 
2022-05-26 05:23:06- epoch: 368/512 - train loss : 3.173 - nce : 3.17 - coor(l/w) : 0.01/0.06 - mask(l/w) : 0.00/0.06  
current lr: 0.002071 
2022-05-26 05:25:58- epoch: 369/512 - train loss : 3.174 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.002046 
2022-05-26 05:28:50- epoch: 370/512 - train loss : 3.174 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.002022 
2022-05-26 05:31:42- epoch: 371/512 - train loss : 3.175 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001997 
2022-05-26 05:34:34- epoch: 372/512 - train loss : 3.173 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001972 
2022-05-26 05:37:27- epoch: 373/512 - train loss : 3.174 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001948 
2022-05-26 05:40:17- epoch: 374/512 - train loss : 3.173 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001924 
2022-05-26 05:43:11- epoch: 375/512 - train loss : 3.173 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.001900 
2022-05-26 05:46:06- epoch: 376/512 - train loss : 3.174 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001876 
2022-05-26 05:49:02- epoch: 377/512 - train loss : 3.173 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001852 
2022-05-26 05:51:57- epoch: 378/512 - train loss : 3.173 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001828 
2022-05-26 05:54:53- epoch: 379/512 - train loss : 3.173 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.001804 
2022-05-26 05:57:50- epoch: 380/512 - train loss : 3.173 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001781 
2022-05-26 06:00:46- epoch: 381/512 - train loss : 3.172 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.001757 
2022-05-26 06:03:44- epoch: 382/512 - train loss : 3.172 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001734 
2022-05-26 06:06:41- epoch: 383/512 - train loss : 3.172 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001711 
2022-05-26 06:09:38- epoch: 384/512 - train loss : 3.172 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.001688 
2022-05-26 06:12:36- epoch: 385/512 - train loss : 3.170 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.001665 
2022-05-26 06:15:34- epoch: 386/512 - train loss : 3.172 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001642 
2022-05-26 06:18:32- epoch: 387/512 - train loss : 3.172 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001620 
2022-05-26 06:21:27- epoch: 388/512 - train loss : 3.172 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001597 
2022-05-26 06:24:25- epoch: 389/512 - train loss : 3.172 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001575 
2022-05-26 06:27:24- epoch: 390/512 - train loss : 3.171 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001552 
2022-05-26 06:30:23- epoch: 391/512 - train loss : 3.172 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001530 
2022-05-26 06:33:23- epoch: 392/512 - train loss : 3.172 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001508 
2022-05-26 06:36:24- epoch: 393/512 - train loss : 3.171 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001486 
2022-05-26 06:39:24- epoch: 394/512 - train loss : 3.171 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001464 
2022-05-26 06:42:25- epoch: 395/512 - train loss : 3.171 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001443 
2022-05-26 06:45:26- epoch: 396/512 - train loss : 3.171 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001421 
2022-05-26 06:48:23- epoch: 397/512 - train loss : 3.170 - nce : 3.17 - coor(l/w) : 0.00/0.07 - mask(l/w) : 0.00/0.07  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.001400 
2022-05-26 06:51:20- epoch: 398/512 - train loss : 3.171 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001379 
2022-05-26 06:54:17- epoch: 399/512 - train loss : 3.170 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.001358 
2022-05-26 06:57:15- epoch: 400/512 - train loss : 3.170 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.001337 
2022-05-26 07:00:13- epoch: 401/512 - train loss : 3.170 - nce : 3.17 - coor(l/w) : 0.00/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001316 
2022-05-26 07:03:11- epoch: 402/512 - train loss : 3.171 - nce : 3.17 - coor(l/w) : 0.00/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001295 
2022-05-26 07:06:09- epoch: 403/512 - train loss : 3.170 - nce : 3.17 - coor(l/w) : 0.00/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001275 
2022-05-26 07:09:08- epoch: 404/512 - train loss : 3.170 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.001254 
2022-05-26 07:12:08- epoch: 405/512 - train loss : 3.170 - nce : 3.17 - coor(l/w) : 0.00/0.07 - mask(l/w) : 0.00/0.07  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.001234 
2022-05-26 07:15:07- epoch: 406/512 - train loss : 3.170 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001214 
2022-05-26 07:18:07- epoch: 407/512 - train loss : 3.169 - nce : 3.17 - coor(l/w) : 0.00/0.07 - mask(l/w) : 0.00/0.07  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.001194 
2022-05-26 07:21:08- epoch: 408/512 - train loss : 3.170 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001174 
2022-05-26 07:24:09- epoch: 409/512 - train loss : 3.169 - nce : 3.17 - coor(l/w) : 0.01/0.07 - mask(l/w) : 0.00/0.07  
current lr: 0.001154 
2022-05-26 07:27:09- epoch: 410/512 - train loss : 3.169 - nce : 3.17 - coor(l/w) : 0.01/0.08 - mask(l/w) : 0.00/0.08  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.001135 
2022-05-26 07:30:11- epoch: 411/512 - train loss : 3.169 - nce : 3.17 - coor(l/w) : 0.01/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.001116 
2022-05-26 07:33:12- epoch: 412/512 - train loss : 3.169 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.001096 
2022-05-26 07:36:14- epoch: 413/512 - train loss : 3.169 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.001077 
2022-05-26 07:39:15- epoch: 414/512 - train loss : 3.169 - nce : 3.17 - coor(l/w) : 0.01/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.001058 
2022-05-26 07:42:18- epoch: 415/512 - train loss : 3.169 - nce : 3.17 - coor(l/w) : 0.01/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.001039 
2022-05-26 07:45:21- epoch: 416/512 - train loss : 3.169 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.001021 
2022-05-26 07:48:23- epoch: 417/512 - train loss : 3.169 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.001002 
2022-05-26 07:51:27- epoch: 418/512 - train loss : 3.169 - nce : 3.17 - coor(l/w) : 0.01/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.000984 
2022-05-26 07:54:31- epoch: 419/512 - train loss : 3.169 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.000966 
2022-05-26 07:57:34- epoch: 420/512 - train loss : 3.168 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.000948 
2022-05-26 08:00:39- epoch: 421/512 - train loss : 3.168 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.000930 
2022-05-26 08:03:46- epoch: 422/512 - train loss : 3.169 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.000912 
2022-05-26 08:06:51- epoch: 423/512 - train loss : 3.168 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.000894 
2022-05-26 08:09:56- epoch: 424/512 - train loss : 3.168 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.000877 
2022-05-26 08:13:02- epoch: 425/512 - train loss : 3.168 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.000860 
2022-05-26 08:16:08- epoch: 426/512 - train loss : 3.168 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.000843 
2022-05-26 08:19:16- epoch: 427/512 - train loss : 3.169 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.000826 
2022-05-26 08:22:26- epoch: 428/512 - train loss : 3.168 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.000809 
2022-05-26 08:25:37- epoch: 429/512 - train loss : 3.168 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.000792 
2022-05-26 08:28:48- epoch: 430/512 - train loss : 3.168 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.000776 
2022-05-26 08:31:59- epoch: 431/512 - train loss : 3.167 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.000759 
2022-05-26 08:35:09- epoch: 432/512 - train loss : 3.168 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.000743 
2022-05-26 08:38:20- epoch: 433/512 - train loss : 3.167 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.000727 
2022-05-26 08:41:33- epoch: 434/512 - train loss : 3.168 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.000711 
2022-05-26 08:44:40- epoch: 435/512 - train loss : 3.168 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.000696 
2022-05-26 08:47:50- epoch: 436/512 - train loss : 3.168 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.000680 
2022-05-26 08:50:59- epoch: 437/512 - train loss : 3.167 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.000665 
2022-05-26 08:54:09- epoch: 438/512 - train loss : 3.167 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.000650 
2022-05-26 08:57:21- epoch: 439/512 - train loss : 3.167 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.000635 
2022-05-26 09:00:32- epoch: 440/512 - train loss : 3.167 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.000620 
2022-05-26 09:03:44- epoch: 441/512 - train loss : 3.167 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.000605 
2022-05-26 09:06:55- epoch: 442/512 - train loss : 3.167 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.000590 
2022-05-26 09:10:10- epoch: 443/512 - train loss : 3.167 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.000576 
2022-05-26 09:13:22- epoch: 444/512 - train loss : 3.167 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.000562 
2022-05-26 09:16:37- epoch: 445/512 - train loss : 3.167 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.000548 
2022-05-26 09:19:52- epoch: 446/512 - train loss : 3.167 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.000534 
2022-05-26 09:23:08- epoch: 447/512 - train loss : 3.167 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.000520 
2022-05-26 09:26:25- epoch: 448/512 - train loss : 3.167 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.000507 
2022-05-26 09:29:41- epoch: 449/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.000493 
2022-05-26 09:32:59- epoch: 450/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.000480 
2022-05-26 09:36:16- epoch: 451/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.08 - mask(l/w) : 0.00/0.08  
current lr: 0.000467 
2022-05-26 09:39:34- epoch: 452/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.000454 
2022-05-26 09:42:51- epoch: 453/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000441 
2022-05-26 09:46:09- epoch: 454/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000429 
2022-05-26 09:49:27- epoch: 455/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000417 
2022-05-26 09:52:46- epoch: 456/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.000404 
2022-05-26 09:56:05- epoch: 457/512 - train loss : 3.165 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.000392 
2022-05-26 09:59:21- epoch: 458/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000381 
2022-05-26 10:02:36- epoch: 459/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000369 
2022-05-26 10:05:51- epoch: 460/512 - train loss : 3.167 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000357 
2022-05-26 10:09:08- epoch: 461/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000346 
2022-05-26 10:12:24- epoch: 462/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000335 
2022-05-26 10:15:41- epoch: 463/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000324 
2022-05-26 10:18:57- epoch: 464/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000313 
2022-05-26 10:22:14- epoch: 465/512 - train loss : 3.165 - nce : 3.16 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.000303 
2022-05-26 10:25:32- epoch: 466/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000292 
2022-05-26 10:28:50- epoch: 467/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000282 
2022-05-26 10:32:08- epoch: 468/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000272 
2022-05-26 10:35:26- epoch: 469/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000262 
2022-05-26 10:38:44- epoch: 470/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000252 
2022-05-26 10:42:02- epoch: 471/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000243 
2022-05-26 10:45:21- epoch: 472/512 - train loss : 3.165 - nce : 3.16 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000233 
2022-05-26 10:48:40- epoch: 473/512 - train loss : 3.165 - nce : 3.16 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000224 
2022-05-26 10:51:59- epoch: 474/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000215 
2022-05-26 10:55:18- epoch: 475/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000206 
2022-05-26 10:58:38- epoch: 476/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000198 
2022-05-26 11:01:58- epoch: 477/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000189 
2022-05-26 11:05:19- epoch: 478/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000181 
2022-05-26 11:08:39- epoch: 479/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000173 
2022-05-26 11:12:00- epoch: 480/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000165 
2022-05-26 11:15:22- epoch: 481/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000157 
2022-05-26 11:18:43- epoch: 482/512 - train loss : 3.165 - nce : 3.16 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000150 
2022-05-26 11:22:05- epoch: 483/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000142 
2022-05-26 11:25:27- epoch: 484/512 - train loss : 3.165 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000135 
2022-05-26 11:28:49- epoch: 485/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000128 
2022-05-26 11:32:13- epoch: 486/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000121 
2022-05-26 11:35:35- epoch: 487/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000115 
2022-05-26 11:38:59- epoch: 488/512 - train loss : 3.165 - nce : 3.16 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000108 
2022-05-26 11:42:22- epoch: 489/512 - train loss : 3.165 - nce : 3.16 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.000102 
2022-05-26 11:45:47- epoch: 490/512 - train loss : 3.165 - nce : 3.16 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000096 
2022-05-26 11:49:13- epoch: 491/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000090 
2022-05-26 11:52:37- epoch: 492/512 - train loss : 3.165 - nce : 3.16 - coor(l/w) : 0.00/0.09 - mask(l/w) : 0.00/0.09  
current lr: 0.000084 
2022-05-26 11:56:02- epoch: 493/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.10 - mask(l/w) : 0.00/0.10  
current lr: 0.000079 
2022-05-26 11:59:29- epoch: 494/512 - train loss : 3.165 - nce : 3.16 - coor(l/w) : 0.00/0.10 - mask(l/w) : 0.00/0.10  
current lr: 0.000074 
2022-05-26 12:02:54- epoch: 495/512 - train loss : 3.165 - nce : 3.16 - coor(l/w) : 0.00/0.10 - mask(l/w) : 0.00/0.10  
current lr: 0.000068 
2022-05-26 12:06:20- epoch: 496/512 - train loss : 3.165 - nce : 3.16 - coor(l/w) : 0.00/0.10 - mask(l/w) : 0.00/0.10  
current lr: 0.000063 
2022-05-26 12:09:46- epoch: 497/512 - train loss : 3.165 - nce : 3.16 - coor(l/w) : 0.00/0.10 - mask(l/w) : 0.00/0.10  
current lr: 0.000059 
2022-05-26 12:13:13- epoch: 498/512 - train loss : 3.165 - nce : 3.16 - coor(l/w) : 0.00/0.10 - mask(l/w) : 0.00/0.10  
current lr: 0.000054 
2022-05-26 12:16:40- epoch: 499/512 - train loss : 3.165 - nce : 3.16 - coor(l/w) : 0.00/0.10 - mask(l/w) : 0.00/0.10  
current lr: 0.000050 
2022-05-26 12:20:07- epoch: 500/512 - train loss : 3.165 - nce : 3.16 - coor(l/w) : 0.00/0.10 - mask(l/w) : 0.00/0.10  
current lr: 0.000045 
2022-05-26 12:23:34- epoch: 501/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.10 - mask(l/w) : 0.00/0.10  
current lr: 0.000041 
2022-05-26 12:27:02- epoch: 502/512 - train loss : 3.165 - nce : 3.16 - coor(l/w) : 0.00/0.10 - mask(l/w) : 0.00/0.10  
current lr: 0.000038 
2022-05-26 12:30:30- epoch: 503/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.10 - mask(l/w) : 0.00/0.10  
current lr: 0.000034 
2022-05-26 12:33:59- epoch: 504/512 - train loss : 3.165 - nce : 3.16 - coor(l/w) : 0.00/0.10 - mask(l/w) : 0.00/0.10  
current lr: 0.000030 
2022-05-26 12:37:29- epoch: 505/512 - train loss : 3.165 - nce : 3.16 - coor(l/w) : 0.00/0.10 - mask(l/w) : 0.00/0.10  
current lr: 0.000027 
2022-05-26 12:41:00- epoch: 506/512 - train loss : 3.166 - nce : 3.17 - coor(l/w) : 0.00/0.10 - mask(l/w) : 0.00/0.10  
current lr: 0.000024 
2022-05-26 12:44:30- epoch: 507/512 - train loss : 3.165 - nce : 3.16 - coor(l/w) : 0.00/0.10 - mask(l/w) : 0.00/0.10  
Best pretext loss achieved, saving model to ./record/weight/05251729/resnet_pretext.pth 
current lr: 0.000021 
2022-05-26 12:48:01- epoch: 508/512 - train loss : 3.165 - nce : 3.16 - coor(l/w) : 0.00/0.10 - mask(l/w) : 0.00/0.10  
current lr: 0.000018 
2022-05-26 12:51:32- epoch: 509/512 - train loss : 3.165 - nce : 3.16 - coor(l/w) : 0.00/0.10 - mask(l/w) : 0.00/0.10  
current lr: 0.000016 
2022-05-26 12:55:04- epoch: 510/512 - train loss : 3.165 - nce : 3.16 - coor(l/w) : 0.00/0.10 - mask(l/w) : 0.00/0.10  
current lr: 0.000014 
2022-05-26 12:58:35- epoch: 511/512 - train loss : 3.165 - nce : 3.16 - coor(l/w) : 0.00/0.10 - mask(l/w) : 0.00/0.10  
current lr: 0.000011 
2022-05-26 13:02:07- epoch: 512/512 - train loss : 3.165 - nce : 3.16 - coor(l/w) : 0.00/0.10 - mask(l/w) : 0.00/0.10  
Load best pretext model. 
Successfully load backbone model from ./record/weight/05251729/resnet_pretext.pth 
==================== downstream ==================== 
no downstream model in ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.010000 
2022-05-26 13:02:43- epoch: 1/128 - train loss: 5.173 - train acc: 0.024 - valid loss: 4.976 - valid acc: 0.0342  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.010000 
2022-05-26 13:03:16- epoch: 2/128 - train loss: 4.841 - train acc: 0.034 - valid loss: 4.845 - valid acc: 0.0221  
current lr: 0.010000 
2022-05-26 13:03:50- epoch: 3/128 - train loss: 4.583 - train acc: 0.046 - valid loss: 4.784 - valid acc: 0.0292  
current lr: 0.010000 
2022-05-26 13:04:23- epoch: 4/128 - train loss: 4.382 - train acc: 0.051 - valid loss: 4.580 - valid acc: 0.0385  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.010000 
2022-05-26 13:04:57- epoch: 5/128 - train loss: 4.185 - train acc: 0.072 - valid loss: 4.418 - valid acc: 0.0533  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.010000 
2022-05-26 13:05:31- epoch: 6/128 - train loss: 3.998 - train acc: 0.083 - valid loss: 4.291 - valid acc: 0.0616  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.010000 
2022-05-26 13:06:04- epoch: 7/128 - train loss: 3.844 - train acc: 0.102 - valid loss: 4.149 - valid acc: 0.0734  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.010000 
2022-05-26 13:06:38- epoch: 8/128 - train loss: 3.641 - train acc: 0.120 - valid loss: 3.914 - valid acc: 0.1004  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.010000 
2022-05-26 13:07:12- epoch: 9/128 - train loss: 3.530 - train acc: 0.139 - valid loss: 3.902 - valid acc: 0.1015  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.010000 
2022-05-26 13:07:45- epoch: 10/128 - train loss: 3.354 - train acc: 0.170 - valid loss: 3.643 - valid acc: 0.1219  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.010000 
2022-05-26 13:08:19- epoch: 11/128 - train loss: 3.224 - train acc: 0.184 - valid loss: 3.590 - valid acc: 0.1217  
current lr: 0.010000 
2022-05-26 13:08:53- epoch: 12/128 - train loss: 3.116 - train acc: 0.208 - valid loss: 3.299 - valid acc: 0.1710  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.009994 
2022-05-26 13:09:27- epoch: 13/128 - train loss: 2.976 - train acc: 0.230 - valid loss: 3.428 - valid acc: 0.1741  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.009986 
2022-05-26 13:10:01- epoch: 14/128 - train loss: 2.839 - train acc: 0.256 - valid loss: 3.415 - valid acc: 0.1619  
current lr: 0.009976 
2022-05-26 13:10:34- epoch: 15/128 - train loss: 2.705 - train acc: 0.281 - valid loss: 3.217 - valid acc: 0.2066  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.009962 
2022-05-26 13:11:08- epoch: 16/128 - train loss: 2.628 - train acc: 0.294 - valid loss: 3.159 - valid acc: 0.2043  
current lr: 0.009946 
2022-05-26 13:11:42- epoch: 17/128 - train loss: 2.486 - train acc: 0.329 - valid loss: 3.544 - valid acc: 0.1781  
current lr: 0.009926 
2022-05-26 13:12:15- epoch: 18/128 - train loss: 2.383 - train acc: 0.353 - valid loss: 2.850 - valid acc: 0.2615  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.009904 
2022-05-26 13:12:49- epoch: 19/128 - train loss: 2.277 - train acc: 0.380 - valid loss: 2.755 - valid acc: 0.2922  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.009879 
2022-05-26 13:13:22- epoch: 20/128 - train loss: 2.202 - train acc: 0.393 - valid loss: 2.903 - valid acc: 0.2598  
current lr: 0.009850 
2022-05-26 13:13:56- epoch: 21/128 - train loss: 2.063 - train acc: 0.431 - valid loss: 2.927 - valid acc: 0.2703  
current lr: 0.009819 
2022-05-26 13:14:30- epoch: 22/128 - train loss: 1.964 - train acc: 0.445 - valid loss: 2.906 - valid acc: 0.2836  
current lr: 0.009785 
2022-05-26 13:15:03- epoch: 23/128 - train loss: 1.887 - train acc: 0.462 - valid loss: 2.838 - valid acc: 0.3067  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.009748 
2022-05-26 13:15:37- epoch: 24/128 - train loss: 1.800 - train acc: 0.483 - valid loss: 2.536 - valid acc: 0.3448  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.009708 
2022-05-26 13:16:11- epoch: 25/128 - train loss: 1.723 - train acc: 0.502 - valid loss: 2.316 - valid acc: 0.3785  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.009665 
2022-05-26 13:16:45- epoch: 26/128 - train loss: 1.611 - train acc: 0.542 - valid loss: 3.868 - valid acc: 0.2071  
current lr: 0.009619 
2022-05-26 13:17:19- epoch: 27/128 - train loss: 1.566 - train acc: 0.546 - valid loss: 2.467 - valid acc: 0.3635  
current lr: 0.009571 
2022-05-26 13:17:53- epoch: 28/128 - train loss: 1.447 - train acc: 0.577 - valid loss: 2.592 - valid acc: 0.3555  
current lr: 0.009520 
2022-05-26 13:18:27- epoch: 29/128 - train loss: 1.407 - train acc: 0.586 - valid loss: 2.386 - valid acc: 0.3923  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.009466 
2022-05-26 13:19:01- epoch: 30/128 - train loss: 1.324 - train acc: 0.614 - valid loss: 2.346 - valid acc: 0.3973  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.009410 
2022-05-26 13:19:35- epoch: 31/128 - train loss: 1.266 - train acc: 0.629 - valid loss: 2.374 - valid acc: 0.3937  
current lr: 0.009350 
2022-05-26 13:20:09- epoch: 32/128 - train loss: 1.205 - train acc: 0.645 - valid loss: 2.679 - valid acc: 0.3711  
current lr: 0.009289 
2022-05-26 13:20:43- epoch: 33/128 - train loss: 1.156 - train acc: 0.658 - valid loss: 2.468 - valid acc: 0.3968  
current lr: 0.009224 
2022-05-26 13:21:16- epoch: 34/128 - train loss: 1.108 - train acc: 0.667 - valid loss: 2.140 - valid acc: 0.4517  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.009157 
2022-05-26 13:21:50- epoch: 35/128 - train loss: 1.006 - train acc: 0.703 - valid loss: 2.098 - valid acc: 0.4682  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.009088 
2022-05-26 13:22:24- epoch: 36/128 - train loss: 0.994 - train acc: 0.700 - valid loss: 2.147 - valid acc: 0.4570  
current lr: 0.009016 
2022-05-26 13:22:58- epoch: 37/128 - train loss: 0.978 - train acc: 0.711 - valid loss: 2.200 - valid acc: 0.4524  
current lr: 0.008942 
2022-05-26 13:23:31- epoch: 38/128 - train loss: 0.892 - train acc: 0.734 - valid loss: 2.249 - valid acc: 0.4505  
current lr: 0.008865 
2022-05-26 13:24:05- epoch: 39/128 - train loss: 0.825 - train acc: 0.765 - valid loss: 2.097 - valid acc: 0.4779  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.008786 
2022-05-26 13:24:39- epoch: 40/128 - train loss: 0.782 - train acc: 0.770 - valid loss: 2.186 - valid acc: 0.4553  
current lr: 0.008705 
2022-05-26 13:25:12- epoch: 41/128 - train loss: 0.755 - train acc: 0.772 - valid loss: 2.176 - valid acc: 0.4662  
current lr: 0.008621 
2022-05-26 13:25:46- epoch: 42/128 - train loss: 0.697 - train acc: 0.795 - valid loss: 2.085 - valid acc: 0.4815  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.008536 
2022-05-26 13:26:20- epoch: 43/128 - train loss: 0.657 - train acc: 0.808 - valid loss: 2.038 - valid acc: 0.5035  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.008448 
2022-05-26 13:26:54- epoch: 44/128 - train loss: 0.608 - train acc: 0.823 - valid loss: 2.101 - valid acc: 0.4883  
current lr: 0.008358 
2022-05-26 13:27:27- epoch: 45/128 - train loss: 0.582 - train acc: 0.831 - valid loss: 2.280 - valid acc: 0.4708  
current lr: 0.008266 
2022-05-26 13:28:01- epoch: 46/128 - train loss: 0.606 - train acc: 0.823 - valid loss: 2.175 - valid acc: 0.4836  
current lr: 0.008172 
2022-05-26 13:28:34- epoch: 47/128 - train loss: 0.549 - train acc: 0.840 - valid loss: 2.066 - valid acc: 0.4986  
current lr: 0.008076 
2022-05-26 13:29:08- epoch: 48/128 - train loss: 0.469 - train acc: 0.869 - valid loss: 1.935 - valid acc: 0.5343  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.007978 
2022-05-26 13:29:42- epoch: 49/128 - train loss: 0.443 - train acc: 0.874 - valid loss: 2.039 - valid acc: 0.5148  
current lr: 0.007879 
2022-05-26 13:30:15- epoch: 50/128 - train loss: 0.394 - train acc: 0.890 - valid loss: 2.063 - valid acc: 0.5150  
current lr: 0.007778 
2022-05-26 13:30:49- epoch: 51/128 - train loss: 0.373 - train acc: 0.898 - valid loss: 2.032 - valid acc: 0.5252  
current lr: 0.007675 
2022-05-26 13:31:23- epoch: 52/128 - train loss: 0.353 - train acc: 0.906 - valid loss: 2.145 - valid acc: 0.5083  
current lr: 0.007571 
2022-05-26 13:31:56- epoch: 53/128 - train loss: 0.327 - train acc: 0.911 - valid loss: 1.848 - valid acc: 0.5595  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.007464 
2022-05-26 13:32:31- epoch: 54/128 - train loss: 0.277 - train acc: 0.929 - valid loss: 1.960 - valid acc: 0.5411  
current lr: 0.007357 
2022-05-26 13:33:04- epoch: 55/128 - train loss: 0.269 - train acc: 0.931 - valid loss: 1.973 - valid acc: 0.5404  
current lr: 0.007248 
2022-05-26 13:33:38- epoch: 56/128 - train loss: 0.242 - train acc: 0.941 - valid loss: 1.963 - valid acc: 0.5402  
current lr: 0.007138 
2022-05-26 13:34:11- epoch: 57/128 - train loss: 0.213 - train acc: 0.951 - valid loss: 1.965 - valid acc: 0.5494  
current lr: 0.007026 
2022-05-26 13:34:45- epoch: 58/128 - train loss: 0.196 - train acc: 0.957 - valid loss: 1.957 - valid acc: 0.5488  
current lr: 0.006913 
2022-05-26 13:35:18- epoch: 59/128 - train loss: 0.166 - train acc: 0.965 - valid loss: 1.853 - valid acc: 0.5704  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.006799 
2022-05-26 13:35:52- epoch: 60/128 - train loss: 0.175 - train acc: 0.961 - valid loss: 1.935 - valid acc: 0.5564  
current lr: 0.006684 
2022-05-26 13:36:25- epoch: 61/128 - train loss: 0.169 - train acc: 0.961 - valid loss: 1.883 - valid acc: 0.5723  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.006568 
2022-05-26 13:36:59- epoch: 62/128 - train loss: 0.141 - train acc: 0.972 - valid loss: 1.884 - valid acc: 0.5633  
current lr: 0.006451 
2022-05-26 13:37:32- epoch: 63/128 - train loss: 0.120 - train acc: 0.979 - valid loss: 1.852 - valid acc: 0.5784  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.006334 
2022-05-26 13:38:06- epoch: 64/128 - train loss: 0.112 - train acc: 0.980 - valid loss: 1.812 - valid acc: 0.5870  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.006215 
2022-05-26 13:38:40- epoch: 65/128 - train loss: 0.100 - train acc: 0.983 - valid loss: 1.819 - valid acc: 0.5885  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.006096 
2022-05-26 13:39:13- epoch: 66/128 - train loss: 0.087 - train acc: 0.988 - valid loss: 1.818 - valid acc: 0.5899  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.005975 
2022-05-26 13:39:47- epoch: 67/128 - train loss: 0.082 - train acc: 0.987 - valid loss: 1.823 - valid acc: 0.5868  
current lr: 0.005855 
2022-05-26 13:40:20- epoch: 68/128 - train loss: 0.065 - train acc: 0.993 - valid loss: 1.832 - valid acc: 0.5865  
current lr: 0.005734 
2022-05-26 13:40:55- epoch: 69/128 - train loss: 0.062 - train acc: 0.993 - valid loss: 1.762 - valid acc: 0.6042  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.005612 
2022-05-26 13:41:28- epoch: 70/128 - train loss: 0.049 - train acc: 0.996 - valid loss: 1.777 - valid acc: 0.5946  
current lr: 0.005490 
2022-05-26 13:42:02- epoch: 71/128 - train loss: 0.042 - train acc: 0.997 - valid loss: 1.771 - valid acc: 0.6022  
current lr: 0.005368 
2022-05-26 13:42:35- epoch: 72/128 - train loss: 0.038 - train acc: 0.997 - valid loss: 1.733 - valid acc: 0.6106  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.005245 
2022-05-26 13:43:09- epoch: 73/128 - train loss: 0.035 - train acc: 0.997 - valid loss: 1.739 - valid acc: 0.6113  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.005123 
2022-05-26 13:43:43- epoch: 74/128 - train loss: 0.033 - train acc: 0.998 - valid loss: 1.743 - valid acc: 0.6013  
current lr: 0.005000 
2022-05-26 13:44:16- epoch: 75/128 - train loss: 0.029 - train acc: 0.999 - valid loss: 1.734 - valid acc: 0.6174  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.004877 
2022-05-26 13:44:51- epoch: 76/128 - train loss: 0.031 - train acc: 0.999 - valid loss: 1.737 - valid acc: 0.6106  
current lr: 0.004755 
2022-05-26 13:45:24- epoch: 77/128 - train loss: 0.030 - train acc: 0.998 - valid loss: 1.729 - valid acc: 0.6156  
current lr: 0.004632 
2022-05-26 13:45:58- epoch: 78/128 - train loss: 0.025 - train acc: 0.999 - valid loss: 1.716 - valid acc: 0.6179  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.004510 
2022-05-26 13:46:32- epoch: 79/128 - train loss: 0.026 - train acc: 0.999 - valid loss: 1.705 - valid acc: 0.6179  
current lr: 0.004388 
2022-05-26 13:47:06- epoch: 80/128 - train loss: 0.024 - train acc: 0.999 - valid loss: 1.716 - valid acc: 0.6134  
current lr: 0.004266 
2022-05-26 13:47:39- epoch: 81/128 - train loss: 0.023 - train acc: 1.000 - valid loss: 1.704 - valid acc: 0.6170  
current lr: 0.004145 
2022-05-26 13:48:13- epoch: 82/128 - train loss: 0.022 - train acc: 0.999 - valid loss: 1.703 - valid acc: 0.6200  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.004025 
2022-05-26 13:48:47- epoch: 83/128 - train loss: 0.021 - train acc: 0.999 - valid loss: 1.687 - valid acc: 0.6212  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.003904 
2022-05-26 13:49:20- epoch: 84/128 - train loss: 0.019 - train acc: 1.000 - valid loss: 1.687 - valid acc: 0.6234  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.003785 
2022-05-26 13:49:54- epoch: 85/128 - train loss: 0.019 - train acc: 0.999 - valid loss: 1.688 - valid acc: 0.6237  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.003666 
2022-05-26 13:50:28- epoch: 86/128 - train loss: 0.018 - train acc: 0.999 - valid loss: 1.686 - valid acc: 0.6191  
current lr: 0.003549 
2022-05-26 13:51:02- epoch: 87/128 - train loss: 0.017 - train acc: 1.000 - valid loss: 1.671 - valid acc: 0.6232  
current lr: 0.003432 
2022-05-26 13:51:36- epoch: 88/128 - train loss: 0.017 - train acc: 1.000 - valid loss: 1.679 - valid acc: 0.6194  
current lr: 0.003316 
2022-05-26 13:52:09- epoch: 89/128 - train loss: 0.019 - train acc: 0.999 - valid loss: 1.672 - valid acc: 0.6255  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.003201 
2022-05-26 13:52:44- epoch: 90/128 - train loss: 0.018 - train acc: 1.000 - valid loss: 1.681 - valid acc: 0.6231  
current lr: 0.003087 
2022-05-26 13:53:17- epoch: 91/128 - train loss: 0.016 - train acc: 1.000 - valid loss: 1.675 - valid acc: 0.6237  
current lr: 0.002974 
2022-05-26 13:53:51- epoch: 92/128 - train loss: 0.016 - train acc: 1.000 - valid loss: 1.676 - valid acc: 0.6239  
current lr: 0.002862 
2022-05-26 13:54:25- epoch: 93/128 - train loss: 0.015 - train acc: 1.000 - valid loss: 1.666 - valid acc: 0.6269  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.002752 
2022-05-26 13:54:59- epoch: 94/128 - train loss: 0.015 - train acc: 1.000 - valid loss: 1.656 - valid acc: 0.6269  
current lr: 0.002643 
2022-05-26 13:55:33- epoch: 95/128 - train loss: 0.014 - train acc: 1.000 - valid loss: 1.665 - valid acc: 0.6256  
current lr: 0.002536 
2022-05-26 13:56:07- epoch: 96/128 - train loss: 0.015 - train acc: 0.999 - valid loss: 1.667 - valid acc: 0.6291  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.002429 
2022-05-26 13:56:41- epoch: 97/128 - train loss: 0.014 - train acc: 1.000 - valid loss: 1.656 - valid acc: 0.6279  
current lr: 0.002325 
2022-05-26 13:57:14- epoch: 98/128 - train loss: 0.014 - train acc: 1.000 - valid loss: 1.655 - valid acc: 0.6279  
current lr: 0.002222 
2022-05-26 13:57:48- epoch: 99/128 - train loss: 0.013 - train acc: 1.000 - valid loss: 1.659 - valid acc: 0.6250  
current lr: 0.002121 
2022-05-26 13:58:21- epoch: 100/128 - train loss: 0.015 - train acc: 1.000 - valid loss: 1.662 - valid acc: 0.6281  
current lr: 0.002022 
2022-05-26 13:58:55- epoch: 101/128 - train loss: 0.014 - train acc: 0.999 - valid loss: 1.657 - valid acc: 0.6282  
current lr: 0.001924 
2022-05-26 13:59:29- epoch: 102/128 - train loss: 0.014 - train acc: 1.000 - valid loss: 1.657 - valid acc: 0.6239  
current lr: 0.001828 
2022-05-26 14:00:03- epoch: 103/128 - train loss: 0.014 - train acc: 1.000 - valid loss: 1.656 - valid acc: 0.6237  
current lr: 0.001734 
2022-05-26 14:00:37- epoch: 104/128 - train loss: 0.013 - train acc: 1.000 - valid loss: 1.658 - valid acc: 0.6275  
current lr: 0.001642 
2022-05-26 14:01:10- epoch: 105/128 - train loss: 0.013 - train acc: 1.000 - valid loss: 1.654 - valid acc: 0.6269  
current lr: 0.001552 
2022-05-26 14:01:44- epoch: 106/128 - train loss: 0.013 - train acc: 1.000 - valid loss: 1.656 - valid acc: 0.6265  
current lr: 0.001464 
2022-05-26 14:02:17- epoch: 107/128 - train loss: 0.013 - train acc: 1.000 - valid loss: 1.658 - valid acc: 0.6253  
current lr: 0.001379 
2022-05-26 14:02:51- epoch: 108/128 - train loss: 0.014 - train acc: 0.999 - valid loss: 1.656 - valid acc: 0.6243  
current lr: 0.001295 
2022-05-26 14:03:25- epoch: 109/128 - train loss: 0.013 - train acc: 1.000 - valid loss: 1.647 - valid acc: 0.6286  
current lr: 0.001214 
2022-05-26 14:03:59- epoch: 110/128 - train loss: 0.013 - train acc: 1.000 - valid loss: 1.648 - valid acc: 0.6274  
current lr: 0.001135 
2022-05-26 14:04:32- epoch: 111/128 - train loss: 0.013 - train acc: 1.000 - valid loss: 1.645 - valid acc: 0.6296  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.001058 
2022-05-26 14:05:06- epoch: 112/128 - train loss: 0.012 - train acc: 1.000 - valid loss: 1.644 - valid acc: 0.6294  
current lr: 0.000984 
2022-05-26 14:05:39- epoch: 113/128 - train loss: 0.013 - train acc: 1.000 - valid loss: 1.644 - valid acc: 0.6291  
current lr: 0.000912 
2022-05-26 14:06:13- epoch: 114/128 - train loss: 0.013 - train acc: 1.000 - valid loss: 1.645 - valid acc: 0.6291  
current lr: 0.000843 
2022-05-26 14:06:46- epoch: 115/128 - train loss: 0.013 - train acc: 1.000 - valid loss: 1.644 - valid acc: 0.6320  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.000776 
2022-05-26 14:07:20- epoch: 116/128 - train loss: 0.012 - train acc: 1.000 - valid loss: 1.648 - valid acc: 0.6289  
current lr: 0.000711 
2022-05-26 14:07:54- epoch: 117/128 - train loss: 0.012 - train acc: 1.000 - valid loss: 1.645 - valid acc: 0.6326  
Best accuracy achieved, saving model to ./record/weight/05251729/resnet_downstream.pth 
current lr: 0.000650 
2022-05-26 14:08:28- epoch: 118/128 - train loss: 0.012 - train acc: 1.000 - valid loss: 1.647 - valid acc: 0.6282  
current lr: 0.000590 
2022-05-26 14:09:01- epoch: 119/128 - train loss: 0.012 - train acc: 1.000 - valid loss: 1.645 - valid acc: 0.6279  
current lr: 0.000534 
2022-05-26 14:09:35- epoch: 120/128 - train loss: 0.012 - train acc: 1.000 - valid loss: 1.640 - valid acc: 0.6289  
current lr: 0.000480 
2022-05-26 14:10:09- epoch: 121/128 - train loss: 0.013 - train acc: 1.000 - valid loss: 1.641 - valid acc: 0.6281  
current lr: 0.000429 
2022-05-26 14:10:43- epoch: 122/128 - train loss: 0.012 - train acc: 1.000 - valid loss: 1.642 - valid acc: 0.6279  
current lr: 0.000381 
2022-05-26 14:11:17- epoch: 123/128 - train loss: 0.012 - train acc: 1.000 - valid loss: 1.638 - valid acc: 0.6317  
current lr: 0.000335 
2022-05-26 14:11:52- epoch: 124/128 - train loss: 0.012 - train acc: 1.000 - valid loss: 1.643 - valid acc: 0.6301  
current lr: 0.000292 
2022-05-26 14:12:25- epoch: 125/128 - train loss: 0.012 - train acc: 1.000 - valid loss: 1.640 - valid acc: 0.6281  
current lr: 0.000252 
2022-05-26 14:12:59- epoch: 126/128 - train loss: 0.012 - train acc: 1.000 - valid loss: 1.644 - valid acc: 0.6274  
current lr: 0.000215 
2022-05-26 14:13:32- epoch: 127/128 - train loss: 0.012 - train acc: 1.000 - valid loss: 1.635 - valid acc: 0.6303  
current lr: 0.000181 
2022-05-26 14:14:06- epoch: 128/128 - train loss: 0.012 - train acc: 1.000 - valid loss: 1.636 - valid acc: 0.6320  
