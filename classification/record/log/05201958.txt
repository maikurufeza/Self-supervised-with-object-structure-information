backbone_have_been_modified. 
================ Dataset Info ===================== 
train images: (5994, 3) 
valid images: (5794, 3) 
num classes: 200 
cuda:0 will be used in the training process !!! 
================ info ===================== 
model: resnet 

*** Pretext imformation *** 
batch size: 32 
epoch: 512 
with LIO: False 
M: False 
learning rate: 0.01 
scheduler: cos 
input resolution: 224 
simclr out dim: 128 

*** downstream imformation *** 
batch size: 64 
epoch: 128 
learning rate: 0.1 
scheduler: cos 
input resolution: 224 
pretrain with imagenet: False 

*** record *** 
pretext model save at  ./record/weight/05201958/resnet_pretext.pth 
downstream model save at  ./record/weight/05201958/resnet_downstream.pth 
log write at  ./record/log/05201958.txt 
no model in ./record/weight/05201958/resnet_pretext.pth 
================ pretext ================ 
current lr: 0.010000 
2022-05-20 19:59:39- epoch: 1/512 - train loss : 4.106  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.010000 
2022-05-20 20:00:24- epoch: 2/512 - train loss : 3.971  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.010000 
2022-05-20 20:01:10- epoch: 3/512 - train loss : 3.922  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.010000 
2022-05-20 20:01:57- epoch: 4/512 - train loss : 3.901  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.010000 
2022-05-20 20:02:45- epoch: 5/512 - train loss : 3.864  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.010000 
2022-05-20 20:03:32- epoch: 6/512 - train loss : 3.865  
current lr: 0.010000 
2022-05-20 20:04:19- epoch: 7/512 - train loss : 3.838  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.010000 
2022-05-20 20:05:06- epoch: 8/512 - train loss : 3.826  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.010000 
2022-05-20 20:05:53- epoch: 9/512 - train loss : 3.806  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.010000 
2022-05-20 20:06:40- epoch: 10/512 - train loss : 3.776  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.010000 
2022-05-20 20:07:27- epoch: 11/512 - train loss : 3.739  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.010000 
2022-05-20 20:08:15- epoch: 12/512 - train loss : 3.694  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.010000 
2022-05-20 20:09:03- epoch: 13/512 - train loss : 3.691  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009999 
2022-05-20 20:09:50- epoch: 14/512 - train loss : 3.655  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009998 
2022-05-20 20:10:37- epoch: 15/512 - train loss : 3.638  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009998 
2022-05-20 20:11:24- epoch: 16/512 - train loss : 3.605  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009997 
2022-05-20 20:12:11- epoch: 17/512 - train loss : 3.591  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009995 
2022-05-20 20:12:59- epoch: 18/512 - train loss : 3.575  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009994 
2022-05-20 20:13:46- epoch: 19/512 - train loss : 3.561  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009992 
2022-05-20 20:14:33- epoch: 20/512 - train loss : 3.540  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009991 
2022-05-20 20:15:21- epoch: 21/512 - train loss : 3.520  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009989 
2022-05-20 20:16:08- epoch: 22/512 - train loss : 3.508  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009986 
2022-05-20 20:16:55- epoch: 23/512 - train loss : 3.496  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009984 
2022-05-20 20:17:43- epoch: 24/512 - train loss : 3.490  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009982 
2022-05-20 20:18:30- epoch: 25/512 - train loss : 3.486  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009979 
2022-05-20 20:19:17- epoch: 26/512 - train loss : 3.479  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009976 
2022-05-20 20:20:04- epoch: 27/512 - train loss : 3.484  
current lr: 0.009973 
2022-05-20 20:20:51- epoch: 28/512 - train loss : 3.457  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009970 
2022-05-20 20:21:39- epoch: 29/512 - train loss : 3.458  
current lr: 0.009966 
2022-05-20 20:22:26- epoch: 30/512 - train loss : 3.454  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009962 
2022-05-20 20:23:13- epoch: 31/512 - train loss : 3.443  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009959 
2022-05-20 20:24:00- epoch: 32/512 - train loss : 3.448  
current lr: 0.009955 
2022-05-20 20:24:47- epoch: 33/512 - train loss : 3.440  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009950 
2022-05-20 20:25:34- epoch: 34/512 - train loss : 3.437  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009946 
2022-05-20 20:26:21- epoch: 35/512 - train loss : 3.432  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009941 
2022-05-20 20:27:09- epoch: 36/512 - train loss : 3.424  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009937 
2022-05-20 20:27:56- epoch: 37/512 - train loss : 3.425  
current lr: 0.009932 
2022-05-20 20:28:43- epoch: 38/512 - train loss : 3.420  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009926 
2022-05-20 20:29:30- epoch: 39/512 - train loss : 3.422  
current lr: 0.009921 
2022-05-20 20:30:17- epoch: 40/512 - train loss : 3.418  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009916 
2022-05-20 20:31:05- epoch: 41/512 - train loss : 3.414  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009910 
2022-05-20 20:31:52- epoch: 42/512 - train loss : 3.410  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009904 
2022-05-20 20:32:40- epoch: 43/512 - train loss : 3.406  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009898 
2022-05-20 20:33:27- epoch: 44/512 - train loss : 3.409  
current lr: 0.009892 
2022-05-20 20:34:14- epoch: 45/512 - train loss : 3.406  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009885 
2022-05-20 20:35:01- epoch: 46/512 - train loss : 3.403  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009879 
2022-05-20 20:35:49- epoch: 47/512 - train loss : 3.403  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009872 
2022-05-20 20:36:36- epoch: 48/512 - train loss : 3.399  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009865 
2022-05-20 20:37:23- epoch: 49/512 - train loss : 3.402  
current lr: 0.009858 
2022-05-20 20:38:11- epoch: 50/512 - train loss : 3.395  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009850 
2022-05-20 20:38:57- epoch: 51/512 - train loss : 3.394  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009843 
2022-05-20 20:39:45- epoch: 52/512 - train loss : 3.394  
current lr: 0.009835 
2022-05-20 20:40:32- epoch: 53/512 - train loss : 3.392  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009827 
2022-05-20 20:41:19- epoch: 54/512 - train loss : 3.386  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009819 
2022-05-20 20:42:07- epoch: 55/512 - train loss : 3.388  
current lr: 0.009811 
2022-05-20 20:42:54- epoch: 56/512 - train loss : 3.389  
current lr: 0.009802 
2022-05-20 20:43:42- epoch: 57/512 - train loss : 3.384  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009794 
2022-05-20 20:44:29- epoch: 58/512 - train loss : 3.386  
current lr: 0.009785 
2022-05-20 20:45:16- epoch: 59/512 - train loss : 3.386  
current lr: 0.009776 
2022-05-20 20:46:03- epoch: 60/512 - train loss : 3.386  
current lr: 0.009767 
2022-05-20 20:46:50- epoch: 61/512 - train loss : 3.382  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009757 
2022-05-20 20:47:38- epoch: 62/512 - train loss : 3.380  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009748 
2022-05-20 20:48:25- epoch: 63/512 - train loss : 3.378  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009738 
2022-05-20 20:49:13- epoch: 64/512 - train loss : 3.373  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009728 
2022-05-20 20:50:00- epoch: 65/512 - train loss : 3.372  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009718 
2022-05-20 20:50:47- epoch: 66/512 - train loss : 3.379  
current lr: 0.009708 
2022-05-20 20:51:35- epoch: 67/512 - train loss : 3.381  
current lr: 0.009697 
2022-05-20 20:52:22- epoch: 68/512 - train loss : 3.377  
current lr: 0.009687 
2022-05-20 20:53:10- epoch: 69/512 - train loss : 3.372  
current lr: 0.009676 
2022-05-20 20:53:57- epoch: 70/512 - train loss : 3.372  
current lr: 0.009665 
2022-05-20 20:54:44- epoch: 71/512 - train loss : 3.374  
current lr: 0.009654 
2022-05-20 20:55:31- epoch: 72/512 - train loss : 3.371  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009643 
2022-05-20 20:56:18- epoch: 73/512 - train loss : 3.376  
current lr: 0.009631 
2022-05-20 20:57:06- epoch: 74/512 - train loss : 3.373  
current lr: 0.009619 
2022-05-20 20:57:53- epoch: 75/512 - train loss : 3.367  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009608 
2022-05-20 20:58:41- epoch: 76/512 - train loss : 3.370  
current lr: 0.009596 
2022-05-20 20:59:27- epoch: 77/512 - train loss : 3.366  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009583 
2022-05-20 21:00:15- epoch: 78/512 - train loss : 3.363  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009571 
2022-05-20 21:01:02- epoch: 79/512 - train loss : 3.361  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009559 
2022-05-20 21:01:50- epoch: 80/512 - train loss : 3.361  
current lr: 0.009546 
2022-05-20 21:02:37- epoch: 81/512 - train loss : 3.360  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009533 
2022-05-20 21:03:24- epoch: 82/512 - train loss : 3.360  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009520 
2022-05-20 21:04:12- epoch: 83/512 - train loss : 3.354  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009507 
2022-05-20 21:04:59- epoch: 84/512 - train loss : 3.356  
current lr: 0.009493 
2022-05-20 21:05:47- epoch: 85/512 - train loss : 3.353  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009480 
2022-05-20 21:06:34- epoch: 86/512 - train loss : 3.353  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009466 
2022-05-20 21:07:22- epoch: 87/512 - train loss : 3.356  
current lr: 0.009452 
2022-05-20 21:08:09- epoch: 88/512 - train loss : 3.353  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009438 
2022-05-20 21:08:56- epoch: 89/512 - train loss : 3.355  
current lr: 0.009424 
2022-05-20 21:09:43- epoch: 90/512 - train loss : 3.348  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009410 
2022-05-20 21:10:31- epoch: 91/512 - train loss : 3.350  
current lr: 0.009395 
2022-05-20 21:11:18- epoch: 92/512 - train loss : 3.351  
current lr: 0.009380 
2022-05-20 21:12:05- epoch: 93/512 - train loss : 3.351  
current lr: 0.009365 
2022-05-20 21:12:53- epoch: 94/512 - train loss : 3.345  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009350 
2022-05-20 21:13:40- epoch: 95/512 - train loss : 3.346  
current lr: 0.009335 
2022-05-20 21:14:28- epoch: 96/512 - train loss : 3.346  
current lr: 0.009320 
2022-05-20 21:15:15- epoch: 97/512 - train loss : 3.343  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009304 
2022-05-20 21:16:02- epoch: 98/512 - train loss : 3.344  
current lr: 0.009289 
2022-05-20 21:16:50- epoch: 99/512 - train loss : 3.343  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009273 
2022-05-20 21:17:37- epoch: 100/512 - train loss : 3.348  
current lr: 0.009257 
2022-05-20 21:18:25- epoch: 101/512 - train loss : 3.343  
current lr: 0.009241 
2022-05-20 21:19:12- epoch: 102/512 - train loss : 3.342  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009224 
2022-05-20 21:19:59- epoch: 103/512 - train loss : 3.342  
current lr: 0.009208 
2022-05-20 21:20:47- epoch: 104/512 - train loss : 3.341  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009191 
2022-05-20 21:21:34- epoch: 105/512 - train loss : 3.340  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009174 
2022-05-20 21:22:21- epoch: 106/512 - train loss : 3.339  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009157 
2022-05-20 21:23:09- epoch: 107/512 - train loss : 3.339  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009140 
2022-05-20 21:23:56- epoch: 108/512 - train loss : 3.341  
current lr: 0.009123 
2022-05-20 21:24:43- epoch: 109/512 - train loss : 3.341  
current lr: 0.009106 
2022-05-20 21:25:31- epoch: 110/512 - train loss : 3.338  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009088 
2022-05-20 21:26:18- epoch: 111/512 - train loss : 3.338  
current lr: 0.009070 
2022-05-20 21:27:05- epoch: 112/512 - train loss : 3.342  
current lr: 0.009052 
2022-05-20 21:27:52- epoch: 113/512 - train loss : 3.339  
current lr: 0.009034 
2022-05-20 21:28:39- epoch: 114/512 - train loss : 3.335  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.009016 
2022-05-20 21:29:27- epoch: 115/512 - train loss : 3.338  
current lr: 0.008998 
2022-05-20 21:30:14- epoch: 116/512 - train loss : 3.337  
current lr: 0.008979 
2022-05-20 21:31:02- epoch: 117/512 - train loss : 3.340  
current lr: 0.008961 
2022-05-20 21:31:49- epoch: 118/512 - train loss : 3.337  
current lr: 0.008942 
2022-05-20 21:32:37- epoch: 119/512 - train loss : 3.332  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.008923 
2022-05-20 21:33:24- epoch: 120/512 - train loss : 3.333  
current lr: 0.008904 
2022-05-20 21:34:11- epoch: 121/512 - train loss : 3.333  
current lr: 0.008884 
2022-05-20 21:34:59- epoch: 122/512 - train loss : 3.333  
current lr: 0.008865 
2022-05-20 21:35:46- epoch: 123/512 - train loss : 3.331  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.008846 
2022-05-20 21:36:34- epoch: 124/512 - train loss : 3.329  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.008826 
2022-05-20 21:37:21- epoch: 125/512 - train loss : 3.332  
current lr: 0.008806 
2022-05-20 21:38:09- epoch: 126/512 - train loss : 3.333  
current lr: 0.008786 
2022-05-20 21:38:56- epoch: 127/512 - train loss : 3.335  
current lr: 0.008766 
2022-05-20 21:39:43- epoch: 128/512 - train loss : 3.332  
current lr: 0.008746 
2022-05-20 21:40:31- epoch: 129/512 - train loss : 3.331  
current lr: 0.008725 
2022-05-20 21:41:18- epoch: 130/512 - train loss : 3.332  
current lr: 0.008705 
2022-05-20 21:42:05- epoch: 131/512 - train loss : 3.328  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.008684 
2022-05-20 21:42:53- epoch: 132/512 - train loss : 3.330  
current lr: 0.008663 
2022-05-20 21:43:40- epoch: 133/512 - train loss : 3.331  
current lr: 0.008642 
2022-05-20 21:44:27- epoch: 134/512 - train loss : 3.329  
current lr: 0.008621 
2022-05-20 21:45:14- epoch: 135/512 - train loss : 3.331  
current lr: 0.008600 
2022-05-20 21:46:02- epoch: 136/512 - train loss : 3.329  
current lr: 0.008579 
2022-05-20 21:46:49- epoch: 137/512 - train loss : 3.330  
current lr: 0.008557 
2022-05-20 21:47:36- epoch: 138/512 - train loss : 3.333  
current lr: 0.008536 
2022-05-20 21:48:24- epoch: 139/512 - train loss : 3.325  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.008514 
2022-05-20 21:49:11- epoch: 140/512 - train loss : 3.327  
current lr: 0.008492 
2022-05-20 21:49:58- epoch: 141/512 - train loss : 3.328  
current lr: 0.008470 
2022-05-20 21:50:46- epoch: 142/512 - train loss : 3.327  
current lr: 0.008448 
2022-05-20 21:51:33- epoch: 143/512 - train loss : 3.326  
current lr: 0.008425 
2022-05-20 21:52:20- epoch: 144/512 - train loss : 3.325  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.008403 
2022-05-20 21:53:08- epoch: 145/512 - train loss : 3.327  
current lr: 0.008380 
2022-05-20 21:53:55- epoch: 146/512 - train loss : 3.326  
current lr: 0.008358 
2022-05-20 21:54:43- epoch: 147/512 - train loss : 3.329  
current lr: 0.008335 
2022-05-20 21:55:30- epoch: 148/512 - train loss : 3.326  
current lr: 0.008312 
2022-05-20 21:56:17- epoch: 149/512 - train loss : 3.325  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.008289 
2022-05-20 21:57:05- epoch: 150/512 - train loss : 3.321  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.008266 
2022-05-20 21:57:53- epoch: 151/512 - train loss : 3.323  
current lr: 0.008243 
2022-05-20 21:58:40- epoch: 152/512 - train loss : 3.321  
current lr: 0.008219 
2022-05-20 21:59:27- epoch: 153/512 - train loss : 3.322  
current lr: 0.008196 
2022-05-20 22:00:15- epoch: 154/512 - train loss : 3.323  
current lr: 0.008172 
2022-05-20 22:01:02- epoch: 155/512 - train loss : 3.320  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.008148 
2022-05-20 22:01:50- epoch: 156/512 - train loss : 3.320  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.008124 
2022-05-20 22:02:37- epoch: 157/512 - train loss : 3.321  
current lr: 0.008100 
2022-05-20 22:03:25- epoch: 158/512 - train loss : 3.324  
current lr: 0.008076 
2022-05-20 22:04:12- epoch: 159/512 - train loss : 3.324  
current lr: 0.008052 
2022-05-20 22:04:59- epoch: 160/512 - train loss : 3.321  
current lr: 0.008028 
2022-05-20 22:05:46- epoch: 161/512 - train loss : 3.318  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.008003 
2022-05-20 22:06:34- epoch: 162/512 - train loss : 3.324  
current lr: 0.007978 
2022-05-20 22:07:21- epoch: 163/512 - train loss : 3.322  
current lr: 0.007954 
2022-05-20 22:08:08- epoch: 164/512 - train loss : 3.318  
current lr: 0.007929 
2022-05-20 22:08:56- epoch: 165/512 - train loss : 3.317  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.007904 
2022-05-20 22:09:43- epoch: 166/512 - train loss : 3.320  
current lr: 0.007879 
2022-05-20 22:10:30- epoch: 167/512 - train loss : 3.318  
current lr: 0.007854 
2022-05-20 22:11:17- epoch: 168/512 - train loss : 3.319  
current lr: 0.007829 
2022-05-20 22:12:05- epoch: 169/512 - train loss : 3.318  
current lr: 0.007803 
2022-05-20 22:12:53- epoch: 170/512 - train loss : 3.317  
current lr: 0.007778 
2022-05-20 22:13:40- epoch: 171/512 - train loss : 3.319  
current lr: 0.007752 
2022-05-20 22:14:27- epoch: 172/512 - train loss : 3.316  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.007727 
2022-05-20 22:15:14- epoch: 173/512 - train loss : 3.318  
current lr: 0.007701 
2022-05-20 22:16:02- epoch: 174/512 - train loss : 3.316  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.007675 
2022-05-20 22:16:49- epoch: 175/512 - train loss : 3.314  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.007649 
2022-05-20 22:17:37- epoch: 176/512 - train loss : 3.314  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.007623 
2022-05-20 22:18:24- epoch: 177/512 - train loss : 3.320  
current lr: 0.007597 
2022-05-20 22:19:12- epoch: 178/512 - train loss : 3.313  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.007571 
2022-05-20 22:19:59- epoch: 179/512 - train loss : 3.314  
current lr: 0.007544 
2022-05-20 22:20:46- epoch: 180/512 - train loss : 3.314  
current lr: 0.007518 
2022-05-20 22:21:33- epoch: 181/512 - train loss : 3.315  
current lr: 0.007491 
2022-05-20 22:22:20- epoch: 182/512 - train loss : 3.313  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.007464 
2022-05-20 22:23:08- epoch: 183/512 - train loss : 3.318  
current lr: 0.007438 
2022-05-20 22:23:55- epoch: 184/512 - train loss : 3.313  
current lr: 0.007411 
2022-05-20 22:24:43- epoch: 185/512 - train loss : 3.314  
current lr: 0.007384 
2022-05-20 22:25:30- epoch: 186/512 - train loss : 3.313  
current lr: 0.007357 
2022-05-20 22:26:17- epoch: 187/512 - train loss : 3.315  
current lr: 0.007330 
2022-05-20 22:27:04- epoch: 188/512 - train loss : 3.311  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.007303 
2022-05-20 22:27:52- epoch: 189/512 - train loss : 3.309  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.007275 
2022-05-20 22:28:39- epoch: 190/512 - train loss : 3.308  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.007248 
2022-05-20 22:29:27- epoch: 191/512 - train loss : 3.311  
current lr: 0.007221 
2022-05-20 22:30:14- epoch: 192/512 - train loss : 3.307  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.007193 
2022-05-20 22:31:02- epoch: 193/512 - train loss : 3.311  
current lr: 0.007165 
2022-05-20 22:31:50- epoch: 194/512 - train loss : 3.313  
current lr: 0.007138 
2022-05-20 22:32:37- epoch: 195/512 - train loss : 3.309  
current lr: 0.007110 
2022-05-20 22:33:24- epoch: 196/512 - train loss : 3.313  
current lr: 0.007082 
2022-05-20 22:34:11- epoch: 197/512 - train loss : 3.308  
current lr: 0.007054 
2022-05-20 22:34:59- epoch: 198/512 - train loss : 3.314  
current lr: 0.007026 
2022-05-20 22:35:46- epoch: 199/512 - train loss : 3.308  
current lr: 0.006998 
2022-05-20 22:36:33- epoch: 200/512 - train loss : 3.312  
current lr: 0.006970 
2022-05-20 22:37:21- epoch: 201/512 - train loss : 3.308  
current lr: 0.006942 
2022-05-20 22:38:08- epoch: 202/512 - train loss : 3.308  
current lr: 0.006913 
2022-05-20 22:38:56- epoch: 203/512 - train loss : 3.308  
current lr: 0.006885 
2022-05-20 22:39:43- epoch: 204/512 - train loss : 3.308  
current lr: 0.006857 
2022-05-20 22:40:31- epoch: 205/512 - train loss : 3.311  
current lr: 0.006828 
2022-05-20 22:41:18- epoch: 206/512 - train loss : 3.307  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.006799 
2022-05-20 22:42:06- epoch: 207/512 - train loss : 3.306  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.006771 
2022-05-20 22:42:53- epoch: 208/512 - train loss : 3.309  
current lr: 0.006742 
2022-05-20 22:43:40- epoch: 209/512 - train loss : 3.307  
current lr: 0.006713 
2022-05-20 22:44:28- epoch: 210/512 - train loss : 3.305  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.006684 
2022-05-20 22:45:15- epoch: 211/512 - train loss : 3.307  
current lr: 0.006656 
2022-05-20 22:46:02- epoch: 212/512 - train loss : 3.308  
current lr: 0.006627 
2022-05-20 22:46:50- epoch: 213/512 - train loss : 3.308  
current lr: 0.006598 
2022-05-20 22:47:37- epoch: 214/512 - train loss : 3.302  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.006568 
2022-05-20 22:48:25- epoch: 215/512 - train loss : 3.305  
current lr: 0.006539 
2022-05-20 22:49:12- epoch: 216/512 - train loss : 3.306  
current lr: 0.006510 
2022-05-20 22:50:00- epoch: 217/512 - train loss : 3.306  
current lr: 0.006481 
2022-05-20 22:50:47- epoch: 218/512 - train loss : 3.306  
current lr: 0.006451 
2022-05-20 22:51:35- epoch: 219/512 - train loss : 3.305  
current lr: 0.006422 
2022-05-20 22:52:22- epoch: 220/512 - train loss : 3.301  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.006393 
2022-05-20 22:53:09- epoch: 221/512 - train loss : 3.301  
current lr: 0.006363 
2022-05-20 22:53:57- epoch: 222/512 - train loss : 3.302  
current lr: 0.006334 
2022-05-20 22:54:44- epoch: 223/512 - train loss : 3.304  
current lr: 0.006304 
2022-05-20 22:55:32- epoch: 224/512 - train loss : 3.303  
current lr: 0.006274 
2022-05-20 22:56:19- epoch: 225/512 - train loss : 3.301  
current lr: 0.006245 
2022-05-20 22:57:06- epoch: 226/512 - train loss : 3.302  
current lr: 0.006215 
2022-05-20 22:57:54- epoch: 227/512 - train loss : 3.301  
current lr: 0.006185 
2022-05-20 22:58:41- epoch: 228/512 - train loss : 3.301  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.006155 
2022-05-20 22:59:29- epoch: 229/512 - train loss : 3.302  
current lr: 0.006125 
2022-05-20 23:00:16- epoch: 230/512 - train loss : 3.304  
current lr: 0.006096 
2022-05-20 23:01:03- epoch: 231/512 - train loss : 3.303  
current lr: 0.006066 
2022-05-20 23:01:50- epoch: 232/512 - train loss : 3.302  
current lr: 0.006036 
2022-05-20 23:02:38- epoch: 233/512 - train loss : 3.301  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.006006 
2022-05-20 23:03:25- epoch: 234/512 - train loss : 3.301  
current lr: 0.005975 
2022-05-20 23:04:13- epoch: 235/512 - train loss : 3.298  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.005945 
2022-05-20 23:05:00- epoch: 236/512 - train loss : 3.302  
current lr: 0.005915 
2022-05-20 23:05:47- epoch: 237/512 - train loss : 3.297  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.005885 
2022-05-20 23:06:35- epoch: 238/512 - train loss : 3.302  
current lr: 0.005855 
2022-05-20 23:07:22- epoch: 239/512 - train loss : 3.297  
current lr: 0.005825 
2022-05-20 23:08:09- epoch: 240/512 - train loss : 3.295  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.005794 
2022-05-20 23:08:57- epoch: 241/512 - train loss : 3.296  
current lr: 0.005764 
2022-05-20 23:09:44- epoch: 242/512 - train loss : 3.297  
current lr: 0.005734 
2022-05-20 23:10:31- epoch: 243/512 - train loss : 3.298  
current lr: 0.005703 
2022-05-20 23:11:18- epoch: 244/512 - train loss : 3.299  
current lr: 0.005673 
2022-05-20 23:12:05- epoch: 245/512 - train loss : 3.299  
current lr: 0.005642 
2022-05-20 23:12:53- epoch: 246/512 - train loss : 3.298  
current lr: 0.005612 
2022-05-20 23:13:40- epoch: 247/512 - train loss : 3.295  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.005582 
2022-05-20 23:14:28- epoch: 248/512 - train loss : 3.296  
current lr: 0.005551 
2022-05-20 23:15:15- epoch: 249/512 - train loss : 3.293  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.005521 
2022-05-20 23:16:02- epoch: 250/512 - train loss : 3.293  
current lr: 0.005490 
2022-05-20 23:16:49- epoch: 251/512 - train loss : 3.296  
current lr: 0.005460 
2022-05-20 23:17:37- epoch: 252/512 - train loss : 3.295  
current lr: 0.005429 
2022-05-20 23:18:24- epoch: 253/512 - train loss : 3.292  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.005398 
2022-05-20 23:19:12- epoch: 254/512 - train loss : 3.297  
current lr: 0.005368 
2022-05-20 23:19:59- epoch: 255/512 - train loss : 3.294  
current lr: 0.005337 
2022-05-20 23:20:46- epoch: 256/512 - train loss : 3.294  
current lr: 0.005307 
2022-05-20 23:21:34- epoch: 257/512 - train loss : 3.294  
current lr: 0.005276 
2022-05-20 23:22:21- epoch: 258/512 - train loss : 3.292  
current lr: 0.005245 
2022-05-20 23:23:08- epoch: 259/512 - train loss : 3.297  
current lr: 0.005215 
2022-05-20 23:23:56- epoch: 260/512 - train loss : 3.293  
current lr: 0.005184 
2022-05-20 23:24:43- epoch: 261/512 - train loss : 3.292  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.005153 
2022-05-20 23:25:30- epoch: 262/512 - train loss : 3.293  
current lr: 0.005123 
2022-05-20 23:26:17- epoch: 263/512 - train loss : 3.293  
current lr: 0.005092 
2022-05-20 23:27:05- epoch: 264/512 - train loss : 3.293  
current lr: 0.005061 
2022-05-20 23:27:52- epoch: 265/512 - train loss : 3.294  
current lr: 0.005031 
2022-05-20 23:28:39- epoch: 266/512 - train loss : 3.292  
current lr: 0.005000 
2022-05-20 23:29:26- epoch: 267/512 - train loss : 3.291  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.004969 
2022-05-20 23:30:14- epoch: 268/512 - train loss : 3.294  
current lr: 0.004939 
2022-05-20 23:31:01- epoch: 269/512 - train loss : 3.293  
current lr: 0.004908 
2022-05-20 23:31:49- epoch: 270/512 - train loss : 3.292  
current lr: 0.004877 
2022-05-20 23:32:36- epoch: 271/512 - train loss : 3.291  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.004847 
2022-05-20 23:33:24- epoch: 272/512 - train loss : 3.287  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.004816 
2022-05-20 23:34:11- epoch: 273/512 - train loss : 3.290  
current lr: 0.004785 
2022-05-20 23:34:58- epoch: 274/512 - train loss : 3.289  
current lr: 0.004755 
2022-05-20 23:35:46- epoch: 275/512 - train loss : 3.289  
current lr: 0.004724 
2022-05-20 23:36:33- epoch: 276/512 - train loss : 3.289  
current lr: 0.004693 
2022-05-20 23:37:21- epoch: 277/512 - train loss : 3.290  
current lr: 0.004663 
2022-05-20 23:38:08- epoch: 278/512 - train loss : 3.287  
current lr: 0.004632 
2022-05-20 23:38:55- epoch: 279/512 - train loss : 3.285  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.004602 
2022-05-20 23:39:42- epoch: 280/512 - train loss : 3.288  
current lr: 0.004571 
2022-05-20 23:40:30- epoch: 281/512 - train loss : 3.291  
current lr: 0.004540 
2022-05-20 23:41:17- epoch: 282/512 - train loss : 3.288  
current lr: 0.004510 
2022-05-20 23:42:05- epoch: 283/512 - train loss : 3.289  
current lr: 0.004479 
2022-05-20 23:42:52- epoch: 284/512 - train loss : 3.286  
current lr: 0.004449 
2022-05-20 23:43:39- epoch: 285/512 - train loss : 3.287  
current lr: 0.004418 
2022-05-20 23:44:27- epoch: 286/512 - train loss : 3.288  
current lr: 0.004388 
2022-05-20 23:45:14- epoch: 287/512 - train loss : 3.286  
current lr: 0.004358 
2022-05-20 23:46:01- epoch: 288/512 - train loss : 3.285  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.004327 
2022-05-20 23:46:48- epoch: 289/512 - train loss : 3.287  
current lr: 0.004297 
2022-05-20 23:47:35- epoch: 290/512 - train loss : 3.285  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.004266 
2022-05-20 23:48:23- epoch: 291/512 - train loss : 3.280  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.004236 
2022-05-20 23:49:10- epoch: 292/512 - train loss : 3.285  
current lr: 0.004206 
2022-05-20 23:49:57- epoch: 293/512 - train loss : 3.286  
current lr: 0.004175 
2022-05-20 23:50:44- epoch: 294/512 - train loss : 3.285  
current lr: 0.004145 
2022-05-20 23:51:31- epoch: 295/512 - train loss : 3.284  
current lr: 0.004115 
2022-05-20 23:52:18- epoch: 296/512 - train loss : 3.283  
current lr: 0.004085 
2022-05-20 23:53:06- epoch: 297/512 - train loss : 3.285  
current lr: 0.004055 
2022-05-20 23:53:53- epoch: 298/512 - train loss : 3.284  
current lr: 0.004025 
2022-05-20 23:54:40- epoch: 299/512 - train loss : 3.284  
current lr: 0.003994 
2022-05-20 23:55:28- epoch: 300/512 - train loss : 3.283  
current lr: 0.003964 
2022-05-20 23:56:15- epoch: 301/512 - train loss : 3.282  
current lr: 0.003934 
2022-05-20 23:57:03- epoch: 302/512 - train loss : 3.281  
current lr: 0.003904 
2022-05-20 23:57:50- epoch: 303/512 - train loss : 3.284  
current lr: 0.003875 
2022-05-20 23:58:37- epoch: 304/512 - train loss : 3.281  
current lr: 0.003845 
2022-05-20 23:59:25- epoch: 305/512 - train loss : 3.281  
current lr: 0.003815 
2022-05-21 00:00:12- epoch: 306/512 - train loss : 3.280  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.003785 
2022-05-21 00:01:00- epoch: 307/512 - train loss : 3.280  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.003755 
2022-05-21 00:01:47- epoch: 308/512 - train loss : 3.280  
current lr: 0.003726 
2022-05-21 00:02:34- epoch: 309/512 - train loss : 3.279  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.003696 
2022-05-21 00:03:22- epoch: 310/512 - train loss : 3.280  
current lr: 0.003666 
2022-05-21 00:04:09- epoch: 311/512 - train loss : 3.280  
current lr: 0.003637 
2022-05-21 00:04:56- epoch: 312/512 - train loss : 3.279  
current lr: 0.003607 
2022-05-21 00:05:43- epoch: 313/512 - train loss : 3.280  
current lr: 0.003578 
2022-05-21 00:06:31- epoch: 314/512 - train loss : 3.278  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.003549 
2022-05-21 00:07:19- epoch: 315/512 - train loss : 3.278  
current lr: 0.003519 
2022-05-21 00:08:06- epoch: 316/512 - train loss : 3.280  
current lr: 0.003490 
2022-05-21 00:08:53- epoch: 317/512 - train loss : 3.278  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.003461 
2022-05-21 00:09:41- epoch: 318/512 - train loss : 3.276  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.003432 
2022-05-21 00:10:28- epoch: 319/512 - train loss : 3.278  
current lr: 0.003402 
2022-05-21 00:11:16- epoch: 320/512 - train loss : 3.277  
current lr: 0.003373 
2022-05-21 00:12:03- epoch: 321/512 - train loss : 3.278  
current lr: 0.003344 
2022-05-21 00:12:50- epoch: 322/512 - train loss : 3.275  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.003316 
2022-05-21 00:13:37- epoch: 323/512 - train loss : 3.274  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.003287 
2022-05-21 00:14:24- epoch: 324/512 - train loss : 3.274  
current lr: 0.003258 
2022-05-21 00:15:12- epoch: 325/512 - train loss : 3.277  
current lr: 0.003229 
2022-05-21 00:15:59- epoch: 326/512 - train loss : 3.275  
current lr: 0.003201 
2022-05-21 00:16:47- epoch: 327/512 - train loss : 3.274  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.003172 
2022-05-21 00:17:34- epoch: 328/512 - train loss : 3.275  
current lr: 0.003143 
2022-05-21 00:18:22- epoch: 329/512 - train loss : 3.276  
current lr: 0.003115 
2022-05-21 00:19:10- epoch: 330/512 - train loss : 3.272  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.003087 
2022-05-21 00:19:58- epoch: 331/512 - train loss : 3.271  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.003058 
2022-05-21 00:20:45- epoch: 332/512 - train loss : 3.274  
current lr: 0.003030 
2022-05-21 00:21:32- epoch: 333/512 - train loss : 3.274  
current lr: 0.003002 
2022-05-21 00:22:20- epoch: 334/512 - train loss : 3.273  
current lr: 0.002974 
2022-05-21 00:23:07- epoch: 335/512 - train loss : 3.272  
current lr: 0.002946 
2022-05-21 00:23:54- epoch: 336/512 - train loss : 3.271  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.002918 
2022-05-21 00:24:42- epoch: 337/512 - train loss : 3.272  
current lr: 0.002890 
2022-05-21 00:25:29- epoch: 338/512 - train loss : 3.271  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.002862 
2022-05-21 00:26:17- epoch: 339/512 - train loss : 3.273  
current lr: 0.002835 
2022-05-21 00:27:04- epoch: 340/512 - train loss : 3.269  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.002807 
2022-05-21 00:27:52- epoch: 341/512 - train loss : 3.271  
current lr: 0.002779 
2022-05-21 00:28:39- epoch: 342/512 - train loss : 3.271  
current lr: 0.002752 
2022-05-21 00:29:26- epoch: 343/512 - train loss : 3.270  
current lr: 0.002725 
2022-05-21 00:30:13- epoch: 344/512 - train loss : 3.269  
current lr: 0.002697 
2022-05-21 00:31:01- epoch: 345/512 - train loss : 3.271  
current lr: 0.002670 
2022-05-21 00:31:48- epoch: 346/512 - train loss : 3.271  
current lr: 0.002643 
2022-05-21 00:32:35- epoch: 347/512 - train loss : 3.269  
current lr: 0.002616 
2022-05-21 00:33:23- epoch: 348/512 - train loss : 3.269  
current lr: 0.002589 
2022-05-21 00:34:11- epoch: 349/512 - train loss : 3.274  
current lr: 0.002562 
2022-05-21 00:34:58- epoch: 350/512 - train loss : 3.268  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.002536 
2022-05-21 00:35:45- epoch: 351/512 - train loss : 3.268  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.002509 
2022-05-21 00:36:33- epoch: 352/512 - train loss : 3.269  
current lr: 0.002482 
2022-05-21 00:37:20- epoch: 353/512 - train loss : 3.266  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.002456 
2022-05-21 00:38:08- epoch: 354/512 - train loss : 3.267  
current lr: 0.002429 
2022-05-21 00:38:55- epoch: 355/512 - train loss : 3.267  
current lr: 0.002403 
2022-05-21 00:39:43- epoch: 356/512 - train loss : 3.267  
current lr: 0.002377 
2022-05-21 00:40:30- epoch: 357/512 - train loss : 3.267  
current lr: 0.002351 
2022-05-21 00:41:17- epoch: 358/512 - train loss : 3.265  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.002325 
2022-05-21 00:42:05- epoch: 359/512 - train loss : 3.265  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.002299 
2022-05-21 00:42:52- epoch: 360/512 - train loss : 3.265  
current lr: 0.002273 
2022-05-21 00:43:40- epoch: 361/512 - train loss : 3.267  
current lr: 0.002248 
2022-05-21 00:44:27- epoch: 362/512 - train loss : 3.264  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.002222 
2022-05-21 00:45:14- epoch: 363/512 - train loss : 3.265  
current lr: 0.002197 
2022-05-21 00:46:02- epoch: 364/512 - train loss : 3.263  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.002171 
2022-05-21 00:46:49- epoch: 365/512 - train loss : 3.265  
current lr: 0.002146 
2022-05-21 00:47:37- epoch: 366/512 - train loss : 3.265  
current lr: 0.002121 
2022-05-21 00:48:24- epoch: 367/512 - train loss : 3.265  
current lr: 0.002096 
2022-05-21 00:49:12- epoch: 368/512 - train loss : 3.262  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.002071 
2022-05-21 00:49:59- epoch: 369/512 - train loss : 3.264  
current lr: 0.002046 
2022-05-21 00:50:47- epoch: 370/512 - train loss : 3.264  
current lr: 0.002022 
2022-05-21 00:51:34- epoch: 371/512 - train loss : 3.262  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.001997 
2022-05-21 00:52:22- epoch: 372/512 - train loss : 3.260  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.001972 
2022-05-21 00:53:09- epoch: 373/512 - train loss : 3.259  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.001948 
2022-05-21 00:53:57- epoch: 374/512 - train loss : 3.261  
current lr: 0.001924 
2022-05-21 00:54:44- epoch: 375/512 - train loss : 3.263  
current lr: 0.001900 
2022-05-21 00:55:31- epoch: 376/512 - train loss : 3.263  
current lr: 0.001876 
2022-05-21 00:56:19- epoch: 377/512 - train loss : 3.263  
current lr: 0.001852 
2022-05-21 00:57:06- epoch: 378/512 - train loss : 3.261  
current lr: 0.001828 
2022-05-21 00:57:53- epoch: 379/512 - train loss : 3.259  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.001804 
2022-05-21 00:58:41- epoch: 380/512 - train loss : 3.256  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.001781 
2022-05-21 00:59:28- epoch: 381/512 - train loss : 3.259  
current lr: 0.001757 
2022-05-21 01:00:15- epoch: 382/512 - train loss : 3.257  
current lr: 0.001734 
2022-05-21 01:01:03- epoch: 383/512 - train loss : 3.257  
current lr: 0.001711 
2022-05-21 01:01:50- epoch: 384/512 - train loss : 3.256  
current lr: 0.001688 
2022-05-21 01:02:37- epoch: 385/512 - train loss : 3.259  
current lr: 0.001665 
2022-05-21 01:03:24- epoch: 386/512 - train loss : 3.259  
current lr: 0.001642 
2022-05-21 01:04:12- epoch: 387/512 - train loss : 3.259  
current lr: 0.001620 
2022-05-21 01:04:59- epoch: 388/512 - train loss : 3.255  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.001597 
2022-05-21 01:05:46- epoch: 389/512 - train loss : 3.257  
current lr: 0.001575 
2022-05-21 01:06:33- epoch: 390/512 - train loss : 3.255  
current lr: 0.001552 
2022-05-21 01:07:22- epoch: 391/512 - train loss : 3.258  
current lr: 0.001530 
2022-05-21 01:08:09- epoch: 392/512 - train loss : 3.257  
current lr: 0.001508 
2022-05-21 01:08:56- epoch: 393/512 - train loss : 3.257  
current lr: 0.001486 
2022-05-21 01:09:43- epoch: 394/512 - train loss : 3.255  
current lr: 0.001464 
2022-05-21 01:10:31- epoch: 395/512 - train loss : 3.256  
current lr: 0.001443 
2022-05-21 01:11:18- epoch: 396/512 - train loss : 3.255  
current lr: 0.001421 
2022-05-21 01:12:05- epoch: 397/512 - train loss : 3.255  
current lr: 0.001400 
2022-05-21 01:12:53- epoch: 398/512 - train loss : 3.252  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.001379 
2022-05-21 01:13:40- epoch: 399/512 - train loss : 3.255  
current lr: 0.001358 
2022-05-21 01:14:28- epoch: 400/512 - train loss : 3.252  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.001337 
2022-05-21 01:15:15- epoch: 401/512 - train loss : 3.255  
current lr: 0.001316 
2022-05-21 01:16:03- epoch: 402/512 - train loss : 3.254  
current lr: 0.001295 
2022-05-21 01:16:50- epoch: 403/512 - train loss : 3.255  
current lr: 0.001275 
2022-05-21 01:17:37- epoch: 404/512 - train loss : 3.252  
current lr: 0.001254 
2022-05-21 01:18:24- epoch: 405/512 - train loss : 3.254  
current lr: 0.001234 
2022-05-21 01:19:12- epoch: 406/512 - train loss : 3.251  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.001214 
2022-05-21 01:19:59- epoch: 407/512 - train loss : 3.251  
current lr: 0.001194 
2022-05-21 01:20:46- epoch: 408/512 - train loss : 3.252  
current lr: 0.001174 
2022-05-21 01:21:33- epoch: 409/512 - train loss : 3.252  
current lr: 0.001154 
2022-05-21 01:22:20- epoch: 410/512 - train loss : 3.249  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.001135 
2022-05-21 01:23:07- epoch: 411/512 - train loss : 3.250  
current lr: 0.001116 
2022-05-21 01:23:55- epoch: 412/512 - train loss : 3.253  
current lr: 0.001096 
2022-05-21 01:24:42- epoch: 413/512 - train loss : 3.253  
current lr: 0.001077 
2022-05-21 01:25:29- epoch: 414/512 - train loss : 3.251  
current lr: 0.001058 
2022-05-21 01:26:16- epoch: 415/512 - train loss : 3.251  
current lr: 0.001039 
2022-05-21 01:27:03- epoch: 416/512 - train loss : 3.249  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.001021 
2022-05-21 01:27:51- epoch: 417/512 - train loss : 3.250  
current lr: 0.001002 
2022-05-21 01:28:38- epoch: 418/512 - train loss : 3.249  
current lr: 0.000984 
2022-05-21 01:29:25- epoch: 419/512 - train loss : 3.248  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000966 
2022-05-21 01:30:12- epoch: 420/512 - train loss : 3.249  
current lr: 0.000948 
2022-05-21 01:31:00- epoch: 421/512 - train loss : 3.247  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000930 
2022-05-21 01:31:47- epoch: 422/512 - train loss : 3.249  
current lr: 0.000912 
2022-05-21 01:32:34- epoch: 423/512 - train loss : 3.247  
current lr: 0.000894 
2022-05-21 01:33:21- epoch: 424/512 - train loss : 3.251  
current lr: 0.000877 
2022-05-21 01:34:08- epoch: 425/512 - train loss : 3.247  
current lr: 0.000860 
2022-05-21 01:34:56- epoch: 426/512 - train loss : 3.249  
current lr: 0.000843 
2022-05-21 01:35:43- epoch: 427/512 - train loss : 3.248  
current lr: 0.000826 
2022-05-21 01:36:30- epoch: 428/512 - train loss : 3.246  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000809 
2022-05-21 01:37:18- epoch: 429/512 - train loss : 3.246  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000792 
2022-05-21 01:38:05- epoch: 430/512 - train loss : 3.245  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000776 
2022-05-21 01:38:52- epoch: 431/512 - train loss : 3.245  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000759 
2022-05-21 01:39:40- epoch: 432/512 - train loss : 3.245  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000743 
2022-05-21 01:40:27- epoch: 433/512 - train loss : 3.246  
current lr: 0.000727 
2022-05-21 01:41:15- epoch: 434/512 - train loss : 3.245  
current lr: 0.000711 
2022-05-21 01:42:02- epoch: 435/512 - train loss : 3.246  
current lr: 0.000696 
2022-05-21 01:42:49- epoch: 436/512 - train loss : 3.245  
current lr: 0.000680 
2022-05-21 01:43:37- epoch: 437/512 - train loss : 3.246  
current lr: 0.000665 
2022-05-21 01:44:24- epoch: 438/512 - train loss : 3.246  
current lr: 0.000650 
2022-05-21 01:45:11- epoch: 439/512 - train loss : 3.245  
current lr: 0.000635 
2022-05-21 01:45:58- epoch: 440/512 - train loss : 3.243  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000620 
2022-05-21 01:46:45- epoch: 441/512 - train loss : 3.243  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000605 
2022-05-21 01:47:33- epoch: 442/512 - train loss : 3.244  
current lr: 0.000590 
2022-05-21 01:48:20- epoch: 443/512 - train loss : 3.243  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000576 
2022-05-21 01:49:08- epoch: 444/512 - train loss : 3.243  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000562 
2022-05-21 01:49:55- epoch: 445/512 - train loss : 3.243  
current lr: 0.000548 
2022-05-21 01:50:42- epoch: 446/512 - train loss : 3.244  
current lr: 0.000534 
2022-05-21 01:51:29- epoch: 447/512 - train loss : 3.241  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000520 
2022-05-21 01:52:16- epoch: 448/512 - train loss : 3.243  
current lr: 0.000507 
2022-05-21 01:53:03- epoch: 449/512 - train loss : 3.242  
current lr: 0.000493 
2022-05-21 01:53:50- epoch: 450/512 - train loss : 3.242  
current lr: 0.000480 
2022-05-21 01:54:37- epoch: 451/512 - train loss : 3.241  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000467 
2022-05-21 01:55:25- epoch: 452/512 - train loss : 3.242  
current lr: 0.000454 
2022-05-21 01:56:12- epoch: 453/512 - train loss : 3.243  
current lr: 0.000441 
2022-05-21 01:57:00- epoch: 454/512 - train loss : 3.240  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000429 
2022-05-21 01:57:47- epoch: 455/512 - train loss : 3.241  
current lr: 0.000417 
2022-05-21 01:58:34- epoch: 456/512 - train loss : 3.244  
current lr: 0.000404 
2022-05-21 01:59:21- epoch: 457/512 - train loss : 3.242  
current lr: 0.000392 
2022-05-21 02:00:08- epoch: 458/512 - train loss : 3.241  
current lr: 0.000381 
2022-05-21 02:00:55- epoch: 459/512 - train loss : 3.242  
current lr: 0.000369 
2022-05-21 02:01:43- epoch: 460/512 - train loss : 3.242  
current lr: 0.000357 
2022-05-21 02:02:30- epoch: 461/512 - train loss : 3.239  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000346 
2022-05-21 02:03:17- epoch: 462/512 - train loss : 3.239  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000335 
2022-05-21 02:04:04- epoch: 463/512 - train loss : 3.239  
current lr: 0.000324 
2022-05-21 02:04:51- epoch: 464/512 - train loss : 3.240  
current lr: 0.000313 
2022-05-21 02:05:39- epoch: 465/512 - train loss : 3.238  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000303 
2022-05-21 02:06:26- epoch: 466/512 - train loss : 3.240  
current lr: 0.000292 
2022-05-21 02:07:13- epoch: 467/512 - train loss : 3.239  
current lr: 0.000282 
2022-05-21 02:08:00- epoch: 468/512 - train loss : 3.240  
current lr: 0.000272 
2022-05-21 02:08:48- epoch: 469/512 - train loss : 3.239  
current lr: 0.000262 
2022-05-21 02:09:35- epoch: 470/512 - train loss : 3.239  
current lr: 0.000252 
2022-05-21 02:10:22- epoch: 471/512 - train loss : 3.240  
current lr: 0.000243 
2022-05-21 02:11:10- epoch: 472/512 - train loss : 3.238  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000233 
2022-05-21 02:11:57- epoch: 473/512 - train loss : 3.239  
current lr: 0.000224 
2022-05-21 02:12:44- epoch: 474/512 - train loss : 3.237  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000215 
2022-05-21 02:13:32- epoch: 475/512 - train loss : 3.237  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000206 
2022-05-21 02:14:19- epoch: 476/512 - train loss : 3.237  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000198 
2022-05-21 02:15:07- epoch: 477/512 - train loss : 3.240  
current lr: 0.000189 
2022-05-21 02:15:54- epoch: 478/512 - train loss : 3.235  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000181 
2022-05-21 02:16:42- epoch: 479/512 - train loss : 3.238  
current lr: 0.000173 
2022-05-21 02:17:29- epoch: 480/512 - train loss : 3.238  
current lr: 0.000165 
2022-05-21 02:18:16- epoch: 481/512 - train loss : 3.238  
current lr: 0.000157 
2022-05-21 02:19:03- epoch: 482/512 - train loss : 3.237  
current lr: 0.000150 
2022-05-21 02:19:50- epoch: 483/512 - train loss : 3.237  
current lr: 0.000142 
2022-05-21 02:20:38- epoch: 484/512 - train loss : 3.237  
current lr: 0.000135 
2022-05-21 02:21:25- epoch: 485/512 - train loss : 3.236  
current lr: 0.000128 
2022-05-21 02:22:13- epoch: 486/512 - train loss : 3.237  
current lr: 0.000121 
2022-05-21 02:23:00- epoch: 487/512 - train loss : 3.238  
current lr: 0.000115 
2022-05-21 02:23:47- epoch: 488/512 - train loss : 3.236  
current lr: 0.000108 
2022-05-21 02:24:35- epoch: 489/512 - train loss : 3.240  
current lr: 0.000102 
2022-05-21 02:25:22- epoch: 490/512 - train loss : 3.238  
current lr: 0.000096 
2022-05-21 02:26:09- epoch: 491/512 - train loss : 3.236  
current lr: 0.000090 
2022-05-21 02:26:57- epoch: 492/512 - train loss : 3.236  
current lr: 0.000084 
2022-05-21 02:27:44- epoch: 493/512 - train loss : 3.237  
current lr: 0.000079 
2022-05-21 02:28:31- epoch: 494/512 - train loss : 3.237  
current lr: 0.000074 
2022-05-21 02:29:19- epoch: 495/512 - train loss : 3.237  
current lr: 0.000068 
2022-05-21 02:30:06- epoch: 496/512 - train loss : 3.237  
current lr: 0.000063 
2022-05-21 02:30:54- epoch: 497/512 - train loss : 3.236  
current lr: 0.000059 
2022-05-21 02:31:41- epoch: 498/512 - train loss : 3.236  
current lr: 0.000054 
2022-05-21 02:32:28- epoch: 499/512 - train loss : 3.236  
current lr: 0.000050 
2022-05-21 02:33:16- epoch: 500/512 - train loss : 3.237  
current lr: 0.000045 
2022-05-21 02:34:03- epoch: 501/512 - train loss : 3.236  
current lr: 0.000041 
2022-05-21 02:34:51- epoch: 502/512 - train loss : 3.237  
current lr: 0.000038 
2022-05-21 02:35:38- epoch: 503/512 - train loss : 3.237  
current lr: 0.000034 
2022-05-21 02:36:25- epoch: 504/512 - train loss : 3.237  
current lr: 0.000030 
2022-05-21 02:37:13- epoch: 505/512 - train loss : 3.237  
current lr: 0.000027 
2022-05-21 02:38:00- epoch: 506/512 - train loss : 3.236  
current lr: 0.000024 
2022-05-21 02:38:47- epoch: 507/512 - train loss : 3.235  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000021 
2022-05-21 02:39:35- epoch: 508/512 - train loss : 3.236  
current lr: 0.000018 
2022-05-21 02:40:22- epoch: 509/512 - train loss : 3.235  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000016 
2022-05-21 02:41:09- epoch: 510/512 - train loss : 3.236  
current lr: 0.000014 
2022-05-21 02:41:57- epoch: 511/512 - train loss : 3.235  
Best pretext loss achieved, saving model to ./record/weight/05201958/resnet_downstream.pth & resnet_pretext.pth 
current lr: 0.000011 
2022-05-21 02:42:44- epoch: 512/512 - train loss : 3.236  
best pretext weights loaded! epoch:511, acc:0.000, pretext loss:3.235, is pretext:True 
================ downstream ================ 
current lr: 0.100000 
2022-05-21 02:43:24- epoch: 1/128 - train loss: 5.209 - train acc: 0.013 - valid loss: 5.252 - valid acc: 0.0081  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.100000 
2022-05-21 02:44:01- epoch: 2/128 - train loss: 4.970 - train acc: 0.016 - valid loss: 5.163 - valid acc: 0.0152  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.100000 
2022-05-21 02:44:39- epoch: 3/128 - train loss: 4.761 - train acc: 0.025 - valid loss: 4.765 - valid acc: 0.0245  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.100000 
2022-05-21 02:45:16- epoch: 4/128 - train loss: 4.638 - train acc: 0.031 - valid loss: 4.729 - valid acc: 0.0318  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.100000 
2022-05-21 02:45:54- epoch: 5/128 - train loss: 4.552 - train acc: 0.038 - valid loss: 4.530 - valid acc: 0.0445  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.100000 
2022-05-21 02:46:31- epoch: 6/128 - train loss: 4.406 - train acc: 0.051 - valid loss: 4.658 - valid acc: 0.0342  
current lr: 0.100000 
2022-05-21 02:47:09- epoch: 7/128 - train loss: 4.293 - train acc: 0.059 - valid loss: 4.395 - valid acc: 0.0635  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.100000 
2022-05-21 02:47:46- epoch: 8/128 - train loss: 4.206 - train acc: 0.066 - valid loss: 4.198 - valid acc: 0.0775  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.100000 
2022-05-21 02:48:24- epoch: 9/128 - train loss: 4.087 - train acc: 0.077 - valid loss: 4.154 - valid acc: 0.0839  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.100000 
2022-05-21 02:49:01- epoch: 10/128 - train loss: 4.004 - train acc: 0.089 - valid loss: 4.357 - valid acc: 0.0694  
current lr: 0.100000 
2022-05-21 02:49:39- epoch: 11/128 - train loss: 3.908 - train acc: 0.103 - valid loss: 4.186 - valid acc: 0.0809  
current lr: 0.100000 
2022-05-21 02:50:16- epoch: 12/128 - train loss: 3.820 - train acc: 0.110 - valid loss: 4.506 - valid acc: 0.0671  
current lr: 0.099940 
2022-05-21 02:50:54- epoch: 13/128 - train loss: 3.738 - train acc: 0.123 - valid loss: 4.255 - valid acc: 0.0839  
current lr: 0.099865 
2022-05-21 02:51:31- epoch: 14/128 - train loss: 3.624 - train acc: 0.137 - valid loss: 4.102 - valid acc: 0.1063  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.099759 
2022-05-21 02:52:08- epoch: 15/128 - train loss: 3.546 - train acc: 0.145 - valid loss: 3.969 - valid acc: 0.1027  
current lr: 0.099624 
2022-05-21 02:52:46- epoch: 16/128 - train loss: 3.462 - train acc: 0.162 - valid loss: 3.805 - valid acc: 0.1213  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.099459 
2022-05-21 02:53:23- epoch: 17/128 - train loss: 3.337 - train acc: 0.187 - valid loss: 4.216 - valid acc: 0.0972  
current lr: 0.099264 
2022-05-21 02:54:01- epoch: 18/128 - train loss: 3.290 - train acc: 0.191 - valid loss: 3.902 - valid acc: 0.1300  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.099039 
2022-05-21 02:54:38- epoch: 19/128 - train loss: 3.217 - train acc: 0.204 - valid loss: 3.754 - valid acc: 0.1484  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.098785 
2022-05-21 02:55:16- epoch: 20/128 - train loss: 3.151 - train acc: 0.216 - valid loss: 3.514 - valid acc: 0.1790  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.098502 
2022-05-21 02:55:53- epoch: 21/128 - train loss: 3.051 - train acc: 0.240 - valid loss: 3.556 - valid acc: 0.1738  
current lr: 0.098189 
2022-05-21 02:56:30- epoch: 22/128 - train loss: 2.972 - train acc: 0.256 - valid loss: 3.682 - valid acc: 0.1691  
current lr: 0.097847 
2022-05-21 02:57:08- epoch: 23/128 - train loss: 2.938 - train acc: 0.254 - valid loss: 3.471 - valid acc: 0.1900  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.097476 
2022-05-21 02:57:45- epoch: 24/128 - train loss: 2.864 - train acc: 0.269 - valid loss: 3.422 - valid acc: 0.2071  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.097077 
2022-05-21 02:58:23- epoch: 25/128 - train loss: 2.799 - train acc: 0.290 - valid loss: 3.425 - valid acc: 0.2192  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.096650 
2022-05-21 02:59:00- epoch: 26/128 - train loss: 2.727 - train acc: 0.300 - valid loss: 3.522 - valid acc: 0.2113  
current lr: 0.096194 
2022-05-21 02:59:38- epoch: 27/128 - train loss: 2.670 - train acc: 0.315 - valid loss: 3.553 - valid acc: 0.1949  
current lr: 0.095710 
2022-05-21 03:00:15- epoch: 28/128 - train loss: 2.617 - train acc: 0.315 - valid loss: 3.529 - valid acc: 0.2031  
current lr: 0.095199 
2022-05-21 03:00:52- epoch: 29/128 - train loss: 2.518 - train acc: 0.347 - valid loss: 3.150 - valid acc: 0.2418  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.094661 
2022-05-21 03:01:30- epoch: 30/128 - train loss: 2.456 - train acc: 0.356 - valid loss: 3.580 - valid acc: 0.2104  
current lr: 0.094096 
2022-05-21 03:02:07- epoch: 31/128 - train loss: 2.397 - train acc: 0.372 - valid loss: 3.127 - valid acc: 0.2699  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.093504 
2022-05-21 03:02:45- epoch: 32/128 - train loss: 2.307 - train acc: 0.393 - valid loss: 3.483 - valid acc: 0.2223  
current lr: 0.092886 
2022-05-21 03:03:22- epoch: 33/128 - train loss: 2.275 - train acc: 0.400 - valid loss: 3.919 - valid acc: 0.1985  
current lr: 0.092243 
2022-05-21 03:03:59- epoch: 34/128 - train loss: 2.257 - train acc: 0.394 - valid loss: 2.956 - valid acc: 0.2900  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.091573 
2022-05-21 03:04:37- epoch: 35/128 - train loss: 2.136 - train acc: 0.421 - valid loss: 3.014 - valid acc: 0.2584  
current lr: 0.090879 
2022-05-21 03:05:14- epoch: 36/128 - train loss: 2.150 - train acc: 0.422 - valid loss: 2.858 - valid acc: 0.3107  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.090160 
2022-05-21 03:05:51- epoch: 37/128 - train loss: 2.080 - train acc: 0.435 - valid loss: 2.935 - valid acc: 0.3019  
current lr: 0.089417 
2022-05-21 03:06:28- epoch: 38/128 - train loss: 1.959 - train acc: 0.461 - valid loss: 3.184 - valid acc: 0.2705  
current lr: 0.088651 
2022-05-21 03:07:05- epoch: 39/128 - train loss: 1.930 - train acc: 0.477 - valid loss: 2.996 - valid acc: 0.3065  
current lr: 0.087860 
2022-05-21 03:07:43- epoch: 40/128 - train loss: 1.886 - train acc: 0.483 - valid loss: 3.352 - valid acc: 0.2710  
current lr: 0.087048 
2022-05-21 03:08:20- epoch: 41/128 - train loss: 1.817 - train acc: 0.504 - valid loss: 3.222 - valid acc: 0.2867  
current lr: 0.086212 
2022-05-21 03:08:57- epoch: 42/128 - train loss: 1.784 - train acc: 0.501 - valid loss: 2.932 - valid acc: 0.3236  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.085355 
2022-05-21 03:09:34- epoch: 43/128 - train loss: 1.766 - train acc: 0.507 - valid loss: 3.180 - valid acc: 0.2906  
current lr: 0.084477 
2022-05-21 03:10:11- epoch: 44/128 - train loss: 1.659 - train acc: 0.540 - valid loss: 2.957 - valid acc: 0.2986  
current lr: 0.083578 
2022-05-21 03:10:49- epoch: 45/128 - train loss: 1.615 - train acc: 0.540 - valid loss: 3.354 - valid acc: 0.2889  
current lr: 0.082659 
2022-05-21 03:11:26- epoch: 46/128 - train loss: 1.565 - train acc: 0.555 - valid loss: 3.246 - valid acc: 0.2938  
current lr: 0.081720 
2022-05-21 03:12:03- epoch: 47/128 - train loss: 1.507 - train acc: 0.572 - valid loss: 3.065 - valid acc: 0.3179  
current lr: 0.080762 
2022-05-21 03:12:41- epoch: 48/128 - train loss: 1.475 - train acc: 0.586 - valid loss: 3.012 - valid acc: 0.3065  
current lr: 0.079785 
2022-05-21 03:13:18- epoch: 49/128 - train loss: 1.409 - train acc: 0.595 - valid loss: 3.179 - valid acc: 0.3076  
current lr: 0.078790 
2022-05-21 03:13:55- epoch: 50/128 - train loss: 1.407 - train acc: 0.597 - valid loss: 3.075 - valid acc: 0.3341  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.077779 
2022-05-21 03:14:32- epoch: 51/128 - train loss: 1.327 - train acc: 0.612 - valid loss: 2.845 - valid acc: 0.3517  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.076750 
2022-05-21 03:15:10- epoch: 52/128 - train loss: 1.286 - train acc: 0.629 - valid loss: 2.889 - valid acc: 0.3383  
current lr: 0.075705 
2022-05-21 03:15:48- epoch: 53/128 - train loss: 1.251 - train acc: 0.635 - valid loss: 3.755 - valid acc: 0.2780  
current lr: 0.074645 
2022-05-21 03:16:25- epoch: 54/128 - train loss: 1.268 - train acc: 0.635 - valid loss: 3.008 - valid acc: 0.3303  
current lr: 0.073570 
2022-05-21 03:17:02- epoch: 55/128 - train loss: 1.136 - train acc: 0.667 - valid loss: 3.070 - valid acc: 0.3283  
current lr: 0.072481 
2022-05-21 03:17:39- epoch: 56/128 - train loss: 1.070 - train acc: 0.689 - valid loss: 2.953 - valid acc: 0.3657  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.071378 
2022-05-21 03:18:17- epoch: 57/128 - train loss: 1.064 - train acc: 0.691 - valid loss: 3.050 - valid acc: 0.3385  
current lr: 0.070262 
2022-05-21 03:18:54- epoch: 58/128 - train loss: 1.017 - train acc: 0.708 - valid loss: 3.090 - valid acc: 0.3390  
current lr: 0.069134 
2022-05-21 03:19:31- epoch: 59/128 - train loss: 1.007 - train acc: 0.709 - valid loss: 2.909 - valid acc: 0.3514  
current lr: 0.067995 
2022-05-21 03:20:08- epoch: 60/128 - train loss: 0.938 - train acc: 0.726 - valid loss: 2.986 - valid acc: 0.3519  
current lr: 0.066844 
2022-05-21 03:20:46- epoch: 61/128 - train loss: 0.900 - train acc: 0.736 - valid loss: 2.760 - valid acc: 0.3818  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.065684 
2022-05-21 03:21:23- epoch: 62/128 - train loss: 0.847 - train acc: 0.751 - valid loss: 2.909 - valid acc: 0.3842  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.064514 
2022-05-21 03:22:01- epoch: 63/128 - train loss: 0.802 - train acc: 0.766 - valid loss: 2.862 - valid acc: 0.3928  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.063336 
2022-05-21 03:22:38- epoch: 64/128 - train loss: 0.723 - train acc: 0.785 - valid loss: 2.835 - valid acc: 0.3845  
current lr: 0.062149 
2022-05-21 03:23:15- epoch: 65/128 - train loss: 0.728 - train acc: 0.788 - valid loss: 3.078 - valid acc: 0.3680  
current lr: 0.060955 
2022-05-21 03:23:53- epoch: 66/128 - train loss: 0.685 - train acc: 0.802 - valid loss: 2.877 - valid acc: 0.3870  
current lr: 0.059755 
2022-05-21 03:24:30- epoch: 67/128 - train loss: 0.659 - train acc: 0.809 - valid loss: 2.873 - valid acc: 0.3780  
current lr: 0.058548 
2022-05-21 03:25:07- epoch: 68/128 - train loss: 0.591 - train acc: 0.830 - valid loss: 3.175 - valid acc: 0.3645  
current lr: 0.057337 
2022-05-21 03:25:44- epoch: 69/128 - train loss: 0.567 - train acc: 0.836 - valid loss: 2.942 - valid acc: 0.3830  
current lr: 0.056121 
2022-05-21 03:26:21- epoch: 70/128 - train loss: 0.569 - train acc: 0.835 - valid loss: 2.701 - valid acc: 0.4165  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.054901 
2022-05-21 03:26:59- epoch: 71/128 - train loss: 0.485 - train acc: 0.871 - valid loss: 2.701 - valid acc: 0.4142  
current lr: 0.053678 
2022-05-21 03:27:36- epoch: 72/128 - train loss: 0.431 - train acc: 0.880 - valid loss: 3.410 - valid acc: 0.3517  
current lr: 0.052453 
2022-05-21 03:28:13- epoch: 73/128 - train loss: 0.417 - train acc: 0.884 - valid loss: 2.783 - valid acc: 0.4173  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.051227 
2022-05-21 03:28:50- epoch: 74/128 - train loss: 0.423 - train acc: 0.881 - valid loss: 2.934 - valid acc: 0.3885  
current lr: 0.050000 
2022-05-21 03:29:28- epoch: 75/128 - train loss: 0.452 - train acc: 0.876 - valid loss: 3.179 - valid acc: 0.3652  
current lr: 0.048773 
2022-05-21 03:30:05- epoch: 76/128 - train loss: 0.413 - train acc: 0.888 - valid loss: 2.709 - valid acc: 0.4115  
current lr: 0.047547 
2022-05-21 03:30:42- epoch: 77/128 - train loss: 0.345 - train acc: 0.914 - valid loss: 2.635 - valid acc: 0.4367  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.046322 
2022-05-21 03:31:19- epoch: 78/128 - train loss: 0.298 - train acc: 0.924 - valid loss: 2.688 - valid acc: 0.4232  
current lr: 0.045099 
2022-05-21 03:31:57- epoch: 79/128 - train loss: 0.229 - train acc: 0.950 - valid loss: 2.669 - valid acc: 0.4482  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.043879 
2022-05-21 03:32:34- epoch: 80/128 - train loss: 0.230 - train acc: 0.944 - valid loss: 2.669 - valid acc: 0.4453  
current lr: 0.042663 
2022-05-21 03:33:11- epoch: 81/128 - train loss: 0.209 - train acc: 0.955 - valid loss: 2.658 - valid acc: 0.4455  
current lr: 0.041452 
2022-05-21 03:33:48- epoch: 82/128 - train loss: 0.200 - train acc: 0.957 - valid loss: 2.697 - valid acc: 0.4320  
current lr: 0.040245 
2022-05-21 03:34:25- epoch: 83/128 - train loss: 0.157 - train acc: 0.970 - valid loss: 2.715 - valid acc: 0.4320  
current lr: 0.039045 
2022-05-21 03:35:03- epoch: 84/128 - train loss: 0.118 - train acc: 0.983 - valid loss: 2.416 - valid acc: 0.4860  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.037851 
2022-05-21 03:35:40- epoch: 85/128 - train loss: 0.105 - train acc: 0.985 - valid loss: 2.417 - valid acc: 0.4853  
current lr: 0.036664 
2022-05-21 03:36:17- epoch: 86/128 - train loss: 0.095 - train acc: 0.989 - valid loss: 2.350 - valid acc: 0.4857  
current lr: 0.035486 
2022-05-21 03:36:54- epoch: 87/128 - train loss: 0.075 - train acc: 0.991 - valid loss: 2.277 - valid acc: 0.5005  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.034316 
2022-05-21 03:37:31- epoch: 88/128 - train loss: 0.068 - train acc: 0.995 - valid loss: 2.297 - valid acc: 0.5005  
current lr: 0.033156 
2022-05-21 03:38:08- epoch: 89/128 - train loss: 0.057 - train acc: 0.996 - valid loss: 2.294 - valid acc: 0.5029  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.032005 
2022-05-21 03:38:46- epoch: 90/128 - train loss: 0.059 - train acc: 0.995 - valid loss: 2.244 - valid acc: 0.5066  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.030866 
2022-05-21 03:39:23- epoch: 91/128 - train loss: 0.048 - train acc: 0.998 - valid loss: 2.262 - valid acc: 0.4997  
current lr: 0.029738 
2022-05-21 03:40:00- epoch: 92/128 - train loss: 0.041 - train acc: 0.999 - valid loss: 2.232 - valid acc: 0.5029  
current lr: 0.028622 
2022-05-21 03:40:37- epoch: 93/128 - train loss: 0.036 - train acc: 0.999 - valid loss: 2.170 - valid acc: 0.5174  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.027519 
2022-05-21 03:41:15- epoch: 94/128 - train loss: 0.038 - train acc: 0.999 - valid loss: 2.163 - valid acc: 0.5181  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.026430 
2022-05-21 03:41:52- epoch: 95/128 - train loss: 0.034 - train acc: 0.999 - valid loss: 2.154 - valid acc: 0.5169  
current lr: 0.025355 
2022-05-21 03:42:29- epoch: 96/128 - train loss: 0.032 - train acc: 0.999 - valid loss: 2.131 - valid acc: 0.5185  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.024295 
2022-05-21 03:43:06- epoch: 97/128 - train loss: 0.032 - train acc: 0.999 - valid loss: 2.118 - valid acc: 0.5292  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.023250 
2022-05-21 03:43:44- epoch: 98/128 - train loss: 0.030 - train acc: 0.999 - valid loss: 2.122 - valid acc: 0.5219  
current lr: 0.022221 
2022-05-21 03:44:21- epoch: 99/128 - train loss: 0.031 - train acc: 0.999 - valid loss: 2.096 - valid acc: 0.5262  
current lr: 0.021210 
2022-05-21 03:44:58- epoch: 100/128 - train loss: 0.028 - train acc: 1.000 - valid loss: 2.089 - valid acc: 0.5292  
current lr: 0.020215 
2022-05-21 03:45:35- epoch: 101/128 - train loss: 0.028 - train acc: 0.999 - valid loss: 2.100 - valid acc: 0.5273  
current lr: 0.019238 
2022-05-21 03:46:12- epoch: 102/128 - train loss: 0.028 - train acc: 1.000 - valid loss: 2.065 - valid acc: 0.5293  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.018280 
2022-05-21 03:46:50- epoch: 103/128 - train loss: 0.029 - train acc: 0.999 - valid loss: 2.053 - valid acc: 0.5338  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.017341 
2022-05-21 03:47:27- epoch: 104/128 - train loss: 0.026 - train acc: 1.000 - valid loss: 2.070 - valid acc: 0.5324  
current lr: 0.016422 
2022-05-21 03:48:04- epoch: 105/128 - train loss: 0.025 - train acc: 1.000 - valid loss: 2.052 - valid acc: 0.5323  
current lr: 0.015523 
2022-05-21 03:48:41- epoch: 106/128 - train loss: 0.025 - train acc: 1.000 - valid loss: 2.034 - valid acc: 0.5368  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.014645 
2022-05-21 03:49:19- epoch: 107/128 - train loss: 0.025 - train acc: 1.000 - valid loss: 2.036 - valid acc: 0.5357  
current lr: 0.013788 
2022-05-21 03:49:56- epoch: 108/128 - train loss: 0.024 - train acc: 1.000 - valid loss: 2.026 - valid acc: 0.5347  
current lr: 0.012952 
2022-05-21 03:50:33- epoch: 109/128 - train loss: 0.023 - train acc: 1.000 - valid loss: 2.009 - valid acc: 0.5369  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.012140 
2022-05-21 03:51:10- epoch: 110/128 - train loss: 0.024 - train acc: 1.000 - valid loss: 2.011 - valid acc: 0.5356  
current lr: 0.011349 
2022-05-21 03:51:47- epoch: 111/128 - train loss: 0.023 - train acc: 1.000 - valid loss: 2.000 - valid acc: 0.5390  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.010583 
2022-05-21 03:52:24- epoch: 112/128 - train loss: 0.024 - train acc: 1.000 - valid loss: 2.013 - valid acc: 0.5400  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.009840 
2022-05-21 03:53:02- epoch: 113/128 - train loss: 0.023 - train acc: 1.000 - valid loss: 1.999 - valid acc: 0.5381  
current lr: 0.009121 
2022-05-21 03:53:39- epoch: 114/128 - train loss: 0.023 - train acc: 1.000 - valid loss: 1.997 - valid acc: 0.5373  
current lr: 0.008427 
2022-05-21 03:54:16- epoch: 115/128 - train loss: 0.023 - train acc: 1.000 - valid loss: 1.984 - valid acc: 0.5402  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.007757 
2022-05-21 03:54:53- epoch: 116/128 - train loss: 0.023 - train acc: 1.000 - valid loss: 1.989 - valid acc: 0.5431  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.007114 
2022-05-21 03:55:30- epoch: 117/128 - train loss: 0.022 - train acc: 1.000 - valid loss: 1.990 - valid acc: 0.5423  
current lr: 0.006496 
2022-05-21 03:56:07- epoch: 118/128 - train loss: 0.023 - train acc: 1.000 - valid loss: 1.989 - valid acc: 0.5392  
current lr: 0.005904 
2022-05-21 03:56:44- epoch: 119/128 - train loss: 0.022 - train acc: 1.000 - valid loss: 1.988 - valid acc: 0.5421  
current lr: 0.005339 
2022-05-21 03:57:21- epoch: 120/128 - train loss: 0.023 - train acc: 1.000 - valid loss: 1.983 - valid acc: 0.5404  
current lr: 0.004801 
2022-05-21 03:57:58- epoch: 121/128 - train loss: 0.022 - train acc: 1.000 - valid loss: 1.980 - valid acc: 0.5390  
current lr: 0.004290 
2022-05-21 03:58:36- epoch: 122/128 - train loss: 0.021 - train acc: 1.000 - valid loss: 1.975 - valid acc: 0.5444  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.003806 
2022-05-21 03:59:13- epoch: 123/128 - train loss: 0.022 - train acc: 1.000 - valid loss: 1.983 - valid acc: 0.5423  
current lr: 0.003350 
2022-05-21 03:59:50- epoch: 124/128 - train loss: 0.022 - train acc: 1.000 - valid loss: 1.967 - valid acc: 0.5469  
Best accuracy achieved, saving model to ./record/weight/05201958/resnet_downstream.pth 
current lr: 0.002923 
2022-05-21 04:00:28- epoch: 125/128 - train loss: 0.021 - train acc: 1.000 - valid loss: 1.974 - valid acc: 0.5428  
current lr: 0.002524 
2022-05-21 04:01:05- epoch: 126/128 - train loss: 0.021 - train acc: 1.000 - valid loss: 1.970 - valid acc: 0.5437  
current lr: 0.002153 
2022-05-21 04:01:42- epoch: 127/128 - train loss: 0.021 - train acc: 1.000 - valid loss: 1.972 - valid acc: 0.5440  
current lr: 0.001811 
2022-05-21 04:02:20- epoch: 128/128 - train loss: 0.021 - train acc: 1.000 - valid loss: 1.981 - valid acc: 0.5402  
